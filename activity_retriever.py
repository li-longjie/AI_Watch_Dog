# activity_retriever.py

import json
import os
import logging
from datetime import datetime, timedelta
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
# å‡è®¾ llm_service.py å’Œ config.py åœ¨åŒä¸€é¡¹ç›®è·¯å¾„ä¸‹ï¼Œå¹¶ä¸”å¯ä»¥è¢«å¯¼å…¥
from llm_service import LLMService # å¦‚æœLLMServiceå°è£…äº†è°ƒç”¨é€»è¾‘
from config import APIConfig # ä¸»è¦ç”¨äºLLMServiceåˆå§‹åŒ–ï¼Œå¦‚æœå®ƒéœ€è¦çš„è¯
import dateparser # æ·»åŠ å¯¼å…¥
from collections import defaultdict # å¯¼å…¥defaultdict
import sqlite3 # <--- æ–°å¢ï¼šå¯¼å…¥sqlite3
from typing import List, Dict, Any, Optional
import re # <--- æ–°å¢ï¼šå¯¼å…¥reæ¨¡å—
from dateparser.search import search_dates # <--- ä¿®æ”¹ï¼šç›´æ¥å¯¼å…¥ search_dates
import threading

# --- ChromaDB å’Œ LLM æœåŠ¡ç›¸å…³å¯¼å…¥ ---
# ... (ä¿ç•™æ‚¨ç°æœ‰çš„ChromaDBå’ŒLLMç›¸å…³å¯¼å…¥)
# ä¾‹å¦‚:
import chromadb
from chromadb.utils import embedding_functions
from llm_service import get_llm_response 

# --- é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# æ–‡ä»¶è·¯å¾„é…ç½®
SCREENSHOT_DIR = "screen_recordings" 
# JSONL_FILE = os.path.join(SCREENSHOT_DIR, "screen_activity_log.jsonl") # <--- æ³¨é‡Šæˆ–åˆ é™¤
DATABASE_FILE = os.path.join(SCREENSHOT_DIR, "activity_log.db") # <--- æ–°å¢ï¼šSQLiteæ•°æ®åº“æ–‡ä»¶å

CHROMA_DB_PATH = "chroma_db_activity" # ç»Ÿä¸€æ•°æ®åº“ç›®å½•
COLLECTION_NAME = "screen_activity"

# å…¨å±€å˜é‡ï¼Œç”¨äºè·Ÿè¸ªå·²ç´¢å¼•çš„è®°å½•ï¼Œé¿å…é‡å¤ç´¢å¼•
# å½“ä»æ•°æ®åº“åŠ è½½æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–°çš„æ–¹å¼æ¥è·Ÿè¸ªå·²ç´¢å¼•çš„IDï¼Œä¾‹å¦‚è®°å½•æœ€åç´¢å¼•çš„IDæˆ–æ—¶é—´æˆ³
last_indexed_id = 0 
indexing_lock = threading.Lock()
# æˆ–è€…ï¼Œå¯ä»¥è€ƒè™‘åœ¨æ•°æ®åº“ä¸­å¢åŠ ä¸€ä¸ª is_indexed æ ‡å¿—ï¼Œä½†è¿™ä¼šå¢åŠ å†™å…¥æ—¶çš„å¤æ‚æ€§ã€‚
# ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å…ˆç”¨ last_indexed_idï¼Œå¹¶å‡è®¾IDæ˜¯è‡ªå¢çš„ã€‚

# --- ChromaDB åˆå§‹åŒ– ---
# (ä¿ç•™æ‚¨ç°æœ‰çš„ChromaDBåˆå§‹åŒ–é€»è¾‘)
# ä¾‹å¦‚:
try:
    # ä½¿ç”¨é»˜è®¤çš„SentenceTransformeræ¨¡å‹
    default_ef = embedding_functions.DefaultEmbeddingFunction()
    
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # è·å–æˆ–åˆ›å»ºé›†åˆï¼Œå¹¶æŒ‡å®šåµŒå…¥å‡½æ•°
    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=default_ef
    )
    logging.info(f"æˆåŠŸè¿æ¥åˆ°ChromaDBå¹¶è·å–/åˆ›å»ºé›†åˆ: {COLLECTION_NAME} at path {CHROMA_DB_PATH}")
except Exception as e:
    logging.error(f"ChromaDBåˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
    # åœ¨ChromaDBå¤±è´¥çš„æƒ…å†µä¸‹ï¼Œæ ¸å¿ƒåŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨ï¼Œå¯ä»¥è€ƒè™‘é€€å‡ºæˆ–æä¾›é™çº§åŠŸèƒ½
    collection = None 

# --- æ•°æ®åº“è¾…åŠ©å‡½æ•° ---
def create_db_connection():
    """ åˆ›å»ºä¸€ä¸ªæ•°æ®åº“è¿æ¥åˆ°SQLiteæ•°æ®åº“ """
    conn = None
    try:
        conn = sqlite3.connect(DATABASE_FILE)
        conn.row_factory = sqlite3.Row # è®©æŸ¥è¯¢ç»“æœå¯ä»¥é€šè¿‡åˆ—åè®¿é—®
    except sqlite3.Error as e:
        logging.error(f"è¿æ¥SQLiteæ•°æ®åº“å¤±è´¥ ({DATABASE_FILE}): {e}")
    return conn

# Screenpipe æ•°æ®æ–‡ä»¶
SCREEN_RECORD_DIR = "screen_recordings"
RECORD_DATA_FILE = os.path.join(SCREEN_RECORD_DIR, "screen_activity_log.jsonl")

# ChromaDB é…ç½®
CHROMA_DB_DIR_ACTIVITY = "chroma_db_activity" # æ–°çš„æ•°æ®åº“ç›®å½•ï¼Œé¿å…ä¸rag_serverçš„å†²çª
CHROMA_COLLECTION_NAME_ACTIVITY = "screen_activity"

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (ä¸ rag_server.py ä¿æŒä¸€è‡´)
embeddings = None
activity_vector_store = None

try:
    embeddings = HuggingFaceEmbeddings(
        model_name="Alibaba-NLP/gte-multilingual-base",
        model_kwargs={'device': 'cpu', 'trust_remote_code': True},
        encode_kwargs={'normalize_embeddings': True}
    )
    logging.info("åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
except Exception as e:
    logging.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
    # embeddings ä¼šä¿æŒä¸º None

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
try:
    if embeddings:
        activity_vector_store = Chroma(
            collection_name=CHROMA_COLLECTION_NAME_ACTIVITY,
            embedding_function=embeddings,
            persist_directory=CHROMA_DB_DIR_ACTIVITY
        )
        logging.info(f"[è°ƒè¯•ç‚¹1] æ´»åŠ¨å‘é‡æ•°æ®åº“å®ä¾‹å·²åˆ›å»ºï¼Œç±»å‹ä¸º: {type(activity_vector_store)}, å¯¹è±¡ä¸º: {activity_vector_store}")
        
        # æ˜¾å¼æ£€æŸ¥æ˜¯å¦ä¸ºNoneï¼Œè€Œä¸æ˜¯ä¾èµ–å…¶å¸ƒå°”å€¼
        if activity_vector_store is not None:
            logging.info(f"[è°ƒè¯•ç‚¹2B] activity_vector_store is not Noneã€‚æ´»åŠ¨å‘é‡æ•°æ®åº“å·²è¿æ¥/åˆ›å»ºäº: {CHROMA_DB_DIR_ACTIVITY}")
        else:
            # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºä¸Šé¢å·²ç»èµ‹å€¼äº†
            logging.error("[è°ƒè¯•ç‚¹2B] activity_vector_store is None AFTER Chroma() call. è¿™è¡¨æ˜Chroma()å¯èƒ½è¿”å›äº†Noneæˆ–å†…éƒ¨æœ‰ä¸¥é‡é”™è¯¯ã€‚")
    else:
        logging.error("ç”±äºåµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæœªèƒ½åˆå§‹åŒ–æ´»åŠ¨å‘é‡æ•°æ®åº“ã€‚")
        # activity_vector_store ä¿æŒä¸º None
except Exception as e:
    logging.error(f"è¿æ¥æˆ–åˆ›å»ºæ´»åŠ¨å‘é‡æ•°æ®åº“æ—¶å‘ç”Ÿä¸¥é‡å¼‚å¸¸: {e}", exc_info=True)
    activity_vector_store = None # ç¡®ä¿åœ¨Chromaåˆå§‹åŒ–å¼‚å¸¸æ—¶æ˜ç¡®è®¾ç½®ä¸ºNone

# åˆå§‹åŒ–LLMæœåŠ¡ (å¦‚æœLLMServiceçš„åˆå§‹åŒ–æ¯”è¾ƒç®€å•ï¼Œå¯ä»¥ç›´æ¥åœ¨è¿™é‡Œè¿›è¡Œ)
# å‡è®¾LLMServiceæœ‰ä¸€ä¸ªé™æ€æ–¹æ³•æˆ–åœ¨å®ä¾‹åŒ–æ—¶ä¸éœ€è¦å¤æ‚å‚æ•°
try:
    # è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨çš„LLMServiceå…·ä½“å®ç°æ¥è°ƒæ•´
    # å¦‚æœLLMService.get_responseæ˜¯é™æ€çš„ï¼Œæˆ–è€…ç±»æœ¬èº«å¤„ç†é…ç½®ï¼Œåˆ™å¯èƒ½ä¸éœ€è¦å®ä¾‹åŒ–
    # æˆ–è€…: llm_service_instance = LLMService(api_config=APIConfig) # ç¤ºä¾‹
    logging.info("LLMæœåŠ¡å‡†å¤‡å°±ç»ª (å‡è®¾å…¶å·²é…ç½®æˆ–ä¸ºé™æ€è°ƒç”¨)ã€‚")
except Exception as e:
    logging.error(f"åˆå§‹åŒ–LLMæœåŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


def batch_iterator(data: list, batch_size: int):
    """ä¸€ä¸ªç®€å•çš„è¿­ä»£å™¨ï¼Œç”¨äºå°†åˆ—è¡¨åˆ†æ‰¹ã€‚"""
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]

def load_and_index_activity_data() -> int:
    """
    ä»SQLiteæ•°æ®åº“åŠ è½½è‡ªä¸Šæ¬¡ç´¢å¼•ä»¥æ¥çš„æ–°æ´»åŠ¨è®°å½•ï¼Œå¹¶ä»¥æ‰¹å¤„ç†æ–¹å¼å°†å®ƒä»¬ç´¢å¼•åˆ°ChromaDBã€‚
    è¿”å›æ–°ç´¢å¼•çš„è®°å½•æ•°é‡ã€‚
    """
    global last_indexed_id, collection

    if not indexing_lock.acquire(blocking=False):
        logging.info("ç´¢å¼•æ“ä½œå·²åœ¨å¦ä¸€ä¸ªçº¿ç¨‹ä¸­è¿›è¡Œï¼Œæœ¬æ¬¡è°ƒç”¨è·³è¿‡ã€‚")
        return 0

    try:
        if collection is None:
            logging.error("ChromaDBé›†åˆæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç´¢å¼•æ•°æ®ã€‚")
            return 0

        conn = create_db_connection()
        if not conn:
            return 0

        total_indexed_count = 0
        try:
            cursor = conn.cursor()
            query = f"""
                SELECT id, timestamp, record_type, triggered_by, event_type, 
                       window_title, process_name, app_name, page_title, url,
                       ocr_text, mouse_x, mouse_y, button
                FROM activity_log 
                WHERE id > ? AND ocr_text IS NOT NULL AND ocr_text != ''
                ORDER BY id ASC 
            """ 
            
            cursor.execute(query, (last_indexed_id,))
            records_to_index = cursor.fetchall()

            if not records_to_index:
                logging.info("æ²¡æœ‰æ–°çš„æ´»åŠ¨è®°å½•éœ€è¦ç´¢å¼•ã€‚")
                return 0

            logging.info(f"å‘ç° {len(records_to_index)} æ¡æ–°è®°å½•éœ€è¦ç´¢å¼• (è‡ª ID: {last_indexed_id} ä¹‹å)...")

            # --- åˆ†æ‰¹å¤„ç†é€»è¾‘ ---
            BATCH_SIZE = 4000  # ChromaDBçš„é»˜è®¤æœ€å¤§æ‰¹æ¬¡å¤§å°ä¸º5461ï¼Œä½¿ç”¨ä¸€ä¸ªå®‰å…¨çš„å€¼

            max_id_in_run = last_indexed_id

            for batch in batch_iterator(records_to_index, BATCH_SIZE):
                documents_to_add = []
                metadatas_to_add = []
                ids_to_add = []
                max_id_in_batch = max_id_in_run

                for record_row in batch:
                    record_dict = dict(record_row)
                    
                    # æ„å»ºæ–‡æ¡£å†…å®¹
                    doc_content_parts = []
                    if record_dict.get("app_name") and record_dict["app_name"] != "Unknown":
                        doc_content_parts.append(f"åº”ç”¨: {record_dict['app_name']}")
                    if record_dict.get("window_title") and record_dict["window_title"] != "Unknown":
                        doc_content_parts.append(f"çª—å£: {record_dict['window_title']}")
                    if record_dict.get("page_title"):
                        doc_content_parts.append(f"é¡µé¢: {record_dict['page_title']}")
                    if record_dict.get("url"):
                        doc_content_parts.append(f"é“¾æ¥: {record_dict['url']}")
                    
                    ocr_text = record_dict.get("ocr_text", "")
                    if ocr_text:
                         doc_content_parts.append(f"å†…å®¹: {ocr_text}")
                    
                    document_text = " | ".join(doc_content_parts)
                    
                    if not document_text.strip():
                        logging.warning(f"è®°å½•ID {record_dict['id']} ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ç´¢å¼•ã€‚")
                        if record_dict['id'] > max_id_in_batch:
                            max_id_in_batch = record_dict['id']
                        continue

                    documents_to_add.append(document_text)
                    
                    # æ„å»ºå…ƒæ•°æ®
                    original_timestamp_iso = record_dict.get("timestamp")
                    timestamp_unix_float = None
                    if original_timestamp_iso:
                        try:
                            timestamp_unix_float = datetime.fromisoformat(original_timestamp_iso).timestamp()
                        except ValueError:
                            logging.warning(f"æ— æ•ˆçš„ISOæ—¶é—´æˆ³æ ¼å¼ '{original_timestamp_iso}' å¯¹äºè®°å½•ID {record_dict['id']}. è¯¥è®°å½•å°†æ— æ³•é€šè¿‡ç²¾ç¡®æ—¶é—´è¿‡æ»¤ã€‚")
                    
                    window_title_value = record_dict.get("window_title")

                    temp_metadata = {
                        "timestamp_iso_str": original_timestamp_iso if original_timestamp_iso else "N/A",
                        "record_type": record_dict.get("record_type", "N/A"),
                        "app_name": record_dict.get("app_name", "Unknown"),
                        "window_title_meta": (window_title_value[:200] if isinstance(window_title_value, str) else "N/A"),
                        "url_meta": (record_dict.get("url")[:250] if record_dict.get("url") else "N/A"),
                        "source_db_id": record_dict.get("id")
                    }
                    if timestamp_unix_float is not None:
                        temp_metadata["timestamp_unix_float"] = timestamp_unix_float
                    
                    cleaned_metadata = {}
                    for key, value in temp_metadata.items():
                        if value is None: 
                            cleaned_metadata[key] = "N/A"
                        elif not isinstance(value, (str, int, float, bool)):
                             cleaned_metadata[key] = str(value)
                        else:
                            cleaned_metadata[key] = value

                    metadatas_to_add.append(cleaned_metadata)
                    ids_to_add.append(f"record_{record_dict['id']}")

                    if record_dict['id'] > max_id_in_batch:
                        max_id_in_batch = record_dict['id']
                
                if documents_to_add:
                    try:
                        logging.info(f"æ­£åœ¨å‘ChromaDBæ·»åŠ  {len(documents_to_add)} æ¡è®°å½•çš„æ‰¹æ¬¡...")
                        collection.add(
                            documents=documents_to_add,
                            metadatas=metadatas_to_add,
                            ids=ids_to_add
                        )
                        batch_indexed_count = len(documents_to_add)
                        total_indexed_count += batch_indexed_count
                        last_indexed_id = max_id_in_batch # æ¯ä¸ªæˆåŠŸæ‰¹æ¬¡åæ›´æ–°
                        max_id_in_run = max_id_in_batch
                        logging.info(f"æˆåŠŸæ·»åŠ æ‰¹æ¬¡ã€‚å½“å‰æ€»å…±æ·»åŠ  {total_indexed_count} æ¡è®°å½•ã€‚Last indexed ID æ›´æ–°ä¸º: {last_indexed_id}")
                    except chromadb.errors.InternalError as e_chroma_internal:
                        if "Batch size" in str(e_chroma_internal):
                            logging.error(f"å‘ChromaDBæ·»åŠ æ•°æ®æ—¶å‡ºç°æ‰¹å¤„ç†å¤§å°é”™è¯¯: {e_chroma_internal}", exc_info=True)
                            logging.error(f"å½“å‰æ‰¹æ¬¡å¤§å°: {len(documents_to_add)}ã€‚è¯·è€ƒè™‘å‡å° BATCH_SIZEã€‚")
                        else:
                            logging.error(f"å‘ChromaDBæ·»åŠ æ•°æ®æ—¶å‡ºç°å†…éƒ¨é”™è¯¯: {e_chroma_internal}", exc_info=True)
                        return total_indexed_count # å‘ç”Ÿé”™è¯¯æ—¶åœæ­¢å¹¶è¿”å›å·²æˆåŠŸç´¢å¼•çš„æ•°é‡
                    except Exception as e:
                        logging.error(f"å‘ChromaDBæ·»åŠ æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
                        return total_indexed_count # å‘ç”Ÿé”™è¯¯æ—¶åœæ­¢

        except sqlite3.Error as e:
            logging.error(f"ä»SQLiteæ•°æ®åº“åŠ è½½æ•°æ®ç”¨äºç´¢å¼•æ—¶å‡ºé”™: {e}")
        except Exception as e_global:
            logging.error(f"ç´¢å¼•æ•°æ®è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e_global}", exc_info=True)
        finally:
            if conn:
                conn.close()
                
        return total_indexed_count
    finally:
        indexing_lock.release()

# å…¨å±€å˜é‡ last_indexed_id çš„åˆå§‹åŒ–é€»è¾‘ï¼š
# æˆ‘ä»¬éœ€è¦åœ¨ç¨‹åºå¯åŠ¨æ—¶ï¼Œä»ChromaDBè·å–å·²å­˜åœ¨çš„æœ€å¤§ source_db_idï¼Œæˆ–è€…ä»æ•°æ®åº“è·å–æœ€å¤§IDä½œä¸ºèµ·ç‚¹ã€‚
# ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ˜¯ï¼Œå¦‚æœChromaDBæ˜¯ç©ºçš„ï¼Œlast_indexed_idä»0å¼€å§‹ã€‚
# å¦‚æœChromaDBéç©ºï¼Œå¯ä»¥æŸ¥è¯¢ChromaDBä¸­å·²å­˜åœ¨çš„æœ€å¤§ source_db_id (å¦‚æœä¹‹å‰å­˜å‚¨äº†è¿™ä¸ªå…ƒæ•°æ®)ã€‚
# æˆ–è€…ï¼Œæ›´ç®€å•çš„æ˜¯ï¼Œæ¯æ¬¡å¯åŠ¨æ—¶éƒ½é‡æ–°ç´¢å¼•æœ€è¿‘ä¸€æ®µæ—¶é—´çš„æ•°æ®ï¼ˆæ¯”å¦‚æœ€è¿‘ä¸€å¤©ï¼‰ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´é‡å¤ã€‚
# æš‚æ—¶ï¼Œæˆ‘ä»¬ä¾èµ–äºç¨‹åºé‡å¯å last_indexed_id ä¿æŒï¼ˆå¦‚æœè„šæœ¬ä¸é‡å¯ï¼‰ï¼Œæˆ–è€…ä»0å¼€å§‹ï¼ˆå¦‚æœè„šæœ¬é‡å¯ï¼‰ã€‚
# ä¸€ä¸ªæ›´å¥å£®çš„æ–¹æ³•æ˜¯æŒä¹…åŒ– last_indexed_idï¼Œæˆ–è€…åœ¨å¯åŠ¨æ—¶æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æœ€å¤§IDã€‚

def initialize_last_indexed_id():
    """
    åœ¨ç¨‹åºå¯åŠ¨æ—¶åˆå§‹åŒ– last_indexed_idã€‚
    ä»ChromaDBä¸­å·²å­˜åœ¨çš„è®°å½•å…ƒæ•°æ®é‡Œæ¢å¤ã€‚
    """
    global last_indexed_id, collection
    if collection:
        try:
            # åªè·å–å…ƒæ•°æ®å¯ä»¥æ›´å¿«
            existing_records = collection.get(include=["metadatas"])
            max_db_id = 0
            if existing_records and existing_records['metadatas']:
                for meta in existing_records['metadatas']:
                    if meta and 'source_db_id' in meta and isinstance(meta['source_db_id'], int):
                        if meta['source_db_id'] > max_db_id:
                            max_db_id = meta['source_db_id']
            last_indexed_id = max_db_id
            logging.info(f"ä»ChromaDBæ¢å¤ï¼Œåˆå§‹åŒ– last_indexed_id ä¸º: {last_indexed_id}")
            return
        except Exception as e:
            logging.warning(f"ä»ChromaDBæ¢å¤last_indexed_idå¤±è´¥: {e}. å°†ä½¿ç”¨é»˜è®¤å€¼0.")
    
    last_indexed_id = 0
    logging.info(f"æœªèƒ½ä»ChromaDBæ¢å¤ last_indexed_idï¼Œåˆå§‹åŒ–ä¸º: {last_indexed_id} (å°†å°è¯•ç´¢å¼•æ‰€æœ‰è®°å½•)")

if collection:
    initialize_last_indexed_id()


def index_single_activity_record(record_data: Dict[str, Any]) -> bool:
    """
    ç´¢å¼•å•æ¡æ´»åŠ¨è®°å½•åˆ°ChromaDBã€‚
    record_data: ä¸€ä¸ªåŒ…å«æ´»åŠ¨è®°å½•çš„å­—å…¸ï¼Œåº”åŒ…å« 'id' å’Œ 'ocr_text' ä»¥åŠå…¶ä»–å…ƒæ•°æ®å­—æ®µã€‚
    è¿”å› True è¡¨ç¤ºæˆåŠŸï¼ŒFalse è¡¨ç¤ºå¤±è´¥ã€‚
    """
    global collection, last_indexed_id
    if collection is None:
        logging.error("index_single_activity_record: ChromaDBé›†åˆæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç´¢å¼•æ•°æ®ã€‚")
        return False

    if not record_data or not isinstance(record_data, dict):
        logging.error("index_single_activity_record: æä¾›çš„è®°å½•æ•°æ®æ— æ•ˆã€‚")
        return False

    record_id = record_data.get("id")
    
    # å…³é”®æ£€æŸ¥ï¼šç¡®ä¿ record_id æ˜¯æœ‰æ•ˆçš„æ•´æ•°
    if record_id is None or not isinstance(record_id, int):
        logging.error(f"index_single_activity_record: æ— æ•ˆæˆ–ç¼ºå¤±çš„è®°å½•ID ('{record_id}')ã€‚æ— æ³•ç´¢å¼•ã€‚æ•°æ®: {record_data}")
        return False # ä¸è¿›è¡Œç´¢å¼•

    ocr_text_content = record_data.get("ocr_text", "")

    # é€šå¸¸æˆ‘ä»¬åªç´¢å¼•åŒ…å«æœ‰æ•ˆOCRæ–‡æœ¬çš„è®°å½•
    # å¯¹äº app_switch äº‹ä»¶ï¼Œå…¶ ocr_text æ˜¯ç”Ÿæˆçš„æè¿°ï¼Œä¹Ÿå¯ä»¥è¢«ç´¢å¼•
    if not ocr_text_content and record_data.get("record_type") != "app_switch": # å¦‚æœä¸æ˜¯app_switchä¸”ocrä¸ºç©ºåˆ™è·³è¿‡
        # æˆ–è€…æ ¹æ®æ‚¨çš„ç­–ç•¥å†³å®šæ˜¯å¦ç´¢å¼•ocr_textä¸ºç©ºçš„è®°å½•
        logging.debug(f"è®°å½•ID {record_id} (ç±»å‹: {record_data.get('record_type')}) OCRæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡å•æ¡ç´¢å¼•ã€‚")
        return True # è®¤ä¸ºå¤„ç†å®Œæˆï¼Œä½†ä¸ç´¢å¼•

    # æ„å»ºæ–‡æ¡£å†…å®¹
    doc_content_parts = []
    if record_data.get("app_name") and record_data["app_name"] != "Unknown":
        doc_content_parts.append(f"åº”ç”¨: {record_data['app_name']}")
    if record_data.get("window_title") and record_data["window_title"] != "Unknown":
        doc_content_parts.append(f"çª—å£: {record_data['window_title']}")
    if record_data.get("page_title"):
        doc_content_parts.append(f"é¡µé¢: {record_data['page_title']}")
    if record_data.get("url"):
        doc_content_parts.append(f"é“¾æ¥: {record_data['url']}")
    
    if ocr_text_content:
         doc_content_parts.append(f"å†…å®¹: {ocr_text_content}")
    
    document_text = " | ".join(doc_content_parts)

    if not document_text.strip():
        logging.warning(f"è®°å½•ID {record_id} ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼ˆå•æ¡ç´¢å¼•ï¼‰ï¼Œè·³è¿‡ã€‚")
        return True 

    # æ„å»ºå…ƒæ•°æ®
    original_timestamp_iso = record_data.get("timestamp")
    timestamp_unix_float = None
    if original_timestamp_iso:
        try:
            timestamp_unix_float = datetime.fromisoformat(original_timestamp_iso).timestamp()
        except ValueError:
            logging.warning(f"index_single_activity_record: æ— æ•ˆçš„ISOæ—¶é—´æˆ³æ ¼å¼ '{original_timestamp_iso}' å¯¹äºè®°å½•ID {record_id}.")

    window_title_value = record_data.get("window_title")
    
    temp_metadata = {
        "timestamp_iso_str": original_timestamp_iso if original_timestamp_iso else "N/A",
        "record_type": record_data.get("record_type", "N/A"),
        "app_name": record_data.get("app_name", "Unknown"),
        "window_title_meta": (window_title_value[:200] if isinstance(window_title_value, str) else "N/A"),
        "url_meta": (record_data.get("url")[:250] if record_data.get("url") else "N/A"),
        "source_db_id": record_id 
    }
    if timestamp_unix_float is not None:
        temp_metadata["timestamp_unix_float"] = timestamp_unix_float

    cleaned_metadata = {}
    for key, value in temp_metadata.items():
        if value is None:
            cleaned_metadata[key] = "N/A"
        elif not isinstance(value, (str, int, float, bool)):
             cleaned_metadata[key] = str(value)
        else:
            cleaned_metadata[key] = value
    
    chroma_id = f"record_{record_id}" # ç¡®ä¿IDæ˜¯å­—ç¬¦ä¸²

    try:
        # ä½¿ç”¨ upsert=True å¯ä»¥å®‰å…¨åœ°è¦†ç›–å·²å­˜åœ¨çš„è®°å½•ï¼Œé¿å…å› é‡å¤IDå¯¼è‡´é”™è¯¯
        collection.add(
            documents=[document_text],
            metadatas=[cleaned_metadata],
            ids=[chroma_id]
        )
        logging.info(f"æˆåŠŸå°†è®°å½•ID {record_id} (Chroma ID: {chroma_id}) å•ç‹¬ç´¢å¼•åˆ°ChromaDBã€‚")
        return True
    except Exception as e:
        error_message = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯ChromaDBæŸåç›¸å…³çš„é”™è¯¯
        if "Error in compaction" in error_message or "hnsw segment reader" in error_message:
            logging.error(f"ChromaDBæ•°æ®åº“æŸåé”™è¯¯ï¼Œè®°å½•ID {record_id} ç´¢å¼•å¤±è´¥: {error_message}")
            logging.info("å»ºè®®é‡å¯ç³»ç»Ÿä»¥é‡ç½®æŸåçš„å‘é‡æ•°æ®åº“")
        elif "already exists" in error_message:
            logging.warning(f"è®°å½•ID {record_id} (Chroma ID: {chroma_id}) å·²å­˜åœ¨äºChromaDBä¸­ï¼Œè·³è¿‡ç´¢å¼•")
            return True  # è§†ä¸ºæˆåŠŸï¼Œå› ä¸ºæ•°æ®å·²å­˜åœ¨
        else:
            logging.error(f"å•ç‹¬ç´¢å¼•è®°å½•ID {record_id} (Chroma ID: {chroma_id}) åˆ°ChromaDBæ—¶å‡ºé”™: {e}", exc_info=True)
        return False

# --- å…¶ä»–å‡½æ•°å°†åœ¨è¿™é‡Œé€ä¸ªä¿®æ”¹ --- 

async def query_recent_activity(query_text: str, custom_prompt: Optional[str] = None, minutes_ago: Optional[int] = None) -> str:
    """
    æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¯èƒ½åŒ…å«è‡ªç„¶è¯­è¨€æ—¶é—´æè¿°ï¼‰å’Œè‡ªå®šä¹‰æç¤ºï¼Œ
    æŸ¥è¯¢ChromaDBä¸­çš„æ´»åŠ¨è®°å½•ï¼Œå¹¶ä½¿ç”¨LLMç”Ÿæˆå›ç­”ã€‚
    """
    global collection
    if collection is None:
        return "æŠ±æ­‰ï¼Œå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œè¯­ä¹‰æŸ¥è¯¢ã€‚\n\nğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥é€šè¿‡Webç•Œé¢çš„'æ´»åŠ¨è®°å½•'é¡µé¢æŸ¥çœ‹æœ€è¿‘çš„æ´»åŠ¨æ•°æ®ï¼Œæˆ–è€…è”ç³»ç®¡ç†å‘˜é‡æ–°å¯ç”¨å‘é‡ç´¢å¼•åŠŸèƒ½ã€‚"

    try:
        # 1. ç¡®ä¿æ•°æ®å·²ç´¢å¼•
        new_indexed_count = load_and_index_activity_data()
        if new_indexed_count > 0:
            logging.info(f"æŸ¥è¯¢å‰ï¼Œæ–°ç´¢å¼•äº† {new_indexed_count} æ¡è®°å½•åˆ°ChromaDBã€‚")
        else:
            logging.info("æŸ¥è¯¢å‰ï¼Œæ²¡æœ‰æ–°çš„è®°å½•è¢«ç´¢å¼•åˆ°ChromaDBã€‚")

        # 2. ä»ç”¨æˆ·æŸ¥è¯¢ä¸­è§£ææ—¶é—´èŒƒå›´
        if minutes_ago is not None and isinstance(minutes_ago, int) and minutes_ago > 0 :
            end_time_dt = datetime.now()
            start_time_dt = end_time_dt - timedelta(minutes=minutes_ago)
            logging.info(f"ä½¿ç”¨å‰ç«¯ä¼ é€’çš„å›ºå®šæ—¶é—´èŒƒå›´: {minutes_ago} åˆ†é’Ÿå‰. ä» {start_time_dt.isoformat()} åˆ° {end_time_dt.isoformat()}")
        else:
            start_time_dt, end_time_dt = parse_time_range_from_query(query_text)
        
        logging.info(f"ç”¨äºChromaDBæŸ¥è¯¢çš„æœ€ç»ˆæ—¶é—´èŒƒå›´: ä» {start_time_dt.isoformat()} åˆ° {end_time_dt.isoformat()}")

        # 3. æ„å»ºChromaDBçš„ 'where' è¿‡æ»¤å™¨
        where_filter = {
            "$and": [
                {"timestamp_unix_float": {"$gte": start_time_dt.timestamp()}},
                {"timestamp_unix_float": {"$lte": end_time_dt.timestamp()}}
            ]
        }
        
        logging.info(f"æ„å»ºçš„ChromaDB 'where' è¿‡æ»¤å™¨: {where_filter}")
        
        # 4. ä»ChromaDBæ£€ç´¢ç›¸å…³æ–‡æ¡£
        results = collection.query(
            query_texts=[query_text],
            n_results=30,
            where=where_filter,
            include=["documents", "metadatas", "distances"]
        )
        
        retrieved_count = len(results['documents'][0]) if results and results['documents'] and results['documents'][0] else 0
        logging.info(f"ChromaDBæŸ¥è¯¢åœ¨åº”ç”¨æ—¶é—´è¿‡æ»¤å™¨åè¿”å›äº† {retrieved_count} æ¡æ–‡æ¡£ã€‚")
        
        retrieved_docs = []
        if retrieved_count > 0:
            for i, doc_text in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {}
                distance = results['distances'][0][i] if results['distances'] and results['distances'][0] else float('inf')
                
                doc_info_parts = [
                    f"æ´»åŠ¨è®°å½• (æ—¶é—´: {metadata.get('timestamp_iso_str', 'æœªçŸ¥')}",
                    f"åº”ç”¨: {metadata.get('app_name', 'æœªçŸ¥')}",
                    f"ç±»å‹: {metadata.get('record_type','æœªçŸ¥')}"
                ]
                url_from_meta = metadata.get('url_meta')
                if url_from_meta and url_from_meta != "N/A":
                    doc_info_parts.append(f"URL: {url_from_meta}")

                doc_info_parts.append(f"ç›¸å…³åº¦: {1-distance:.2f})")
                
                doc_info_header = ", ".join(doc_info_parts)
                doc_info = f"{doc_info_header}:\n{doc_text}\n---"
                retrieved_docs.append(doc_info)
        
        if not retrieved_docs:
            return f"æ ¹æ®æ‚¨çš„é—®é¢˜å¹¶åœ¨æŒ‡å®šçš„æ—¶é—´èŒƒå›´ï¼ˆä» {start_time_dt.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {end_time_dt.strftime('%Y-%m-%d %H:%M:%S')}ï¼‰å†…ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ´»åŠ¨è®°å½•ã€‚"

        context_for_llm = "\n".join(retrieved_docs)
        
        # 5. æ„å»ºæœ€ç»ˆçš„æç¤ºè¯å¹¶è°ƒç”¨LLM
        if custom_prompt:
            final_prompt = custom_prompt + f"\n\nä»¥ä¸‹æ˜¯ç›¸å…³çš„å±å¹•æ´»åŠ¨æ‘˜è¦ (å·²æŒ‰æ—¶é—´èŒƒå›´ç­›é€‰)ï¼Œè¯·åŸºäºè¿™äº›ä¿¡æ¯å›ç­”é—®é¢˜:\n{context_for_llm}\n\nç”¨æˆ·çš„é—®é¢˜æ˜¯: {query_text}"
        else:
            final_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…ç­›é€‰çš„å±å¹•æ´»åŠ¨è®°å½•æ‘˜è¦æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¿™äº›è®°å½•å¯èƒ½åŒ…å«ç½‘é¡µURLå’Œé¡µé¢æ ‡é¢˜ã€‚å½“è¢«é—®åŠæµè§ˆè¿‡çš„ç½‘é¡µæ—¶ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨å¹¶æä¾›å…·ä½“çš„URLé“¾æ¥ã€‚
æ´»åŠ¨è®°å½•æ‘˜è¦ (æ—¶é—´èŒƒå›´: {start_time_dt.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {end_time_dt.strftime('%Y-%m-%d %H:%M:%S')}):
{context_for_llm}

ç”¨æˆ·çš„é—®é¢˜æ˜¯: "{query_text}"
è¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯åŒ…å«URLï¼Œè¯·æ¸…æ™°åœ°åˆ—å‡ºURLé“¾æ¥ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œå¯ä»¥è¯´ä¿¡æ¯ä¸è¶³ã€‚è¯·ä¸¥æ ¼åŸºäºæä¾›çš„æ‘˜è¦å†…å®¹ã€‚
"""
        
        logging.debug(f"LLM Final Prompt (éƒ¨åˆ†):\n{final_prompt[:1000]}...")

        llm_response = await get_llm_response(final_prompt)
        return llm_response

    except chromadb.errors.InternalError as e:
        error_message = str(e)
        logging.error(f"æŸ¥è¯¢æ´»åŠ¨è®°å½•æ—¶å‘ç”ŸChromaDBå†…éƒ¨é”™è¯¯: {error_message}", exc_info=True)
        if "Error finding id" in error_message:
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿäº†æ•°æ®åº“é”™è¯¯ (Error finding id)ã€‚è¿™é€šå¸¸æ˜¯ç”±äºå‘é‡æ•°æ®åº“ç´¢å¼•æŸåå¯¼è‡´çš„ã€‚è¯·å°è¯•é‡å¯ç¨‹åºï¼Œç³»ç»Ÿåœ¨å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨å°è¯•ä¿®å¤ç´¢å¼•ã€‚å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œæ‚¨å¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ é™¤ 'chroma_db_activity' ç›®å½•æ¥å¼ºåˆ¶é‡å»ºæ•°æ®åº“ã€‚"
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿäº†æ•°æ®åº“å†…éƒ¨é”™è¯¯: {e}"
    except Exception as e:
        logging.error(f"æŸ¥è¯¢æ´»åŠ¨è®°å½•æˆ–è°ƒç”¨LLMæ—¶å‡ºé”™: {e}", exc_info=True)
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿäº†é”™è¯¯: {e}"


# --- å…¶ä»–å¯èƒ½éœ€è¦ä¿®æ”¹çš„å‡½æ•°ï¼Œä¾‹å¦‚ä¸ç‰¹å®šæ–‡ä»¶æ ¼å¼ç›¸å…³çš„è¾…åŠ©å‡½æ•°ï¼Œç°åœ¨å¯ä»¥ç§»é™¤äº† ---
# ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ç±»ä¼¼ load_jsonl_data çš„å‡½æ•°ï¼Œç°åœ¨ä¸å†éœ€è¦ã€‚

# --- æ—¶é—´è§£æè¾…åŠ©å‡½æ•° (å¦‚æœä¹‹å‰æ²¡æœ‰ï¼Œå¯ä»¥ä¿ç•™æˆ–æ·»åŠ ) ---
def parse_time_range_from_query(query_text: str, default_minutes_ago: int = 1440) -> tuple[datetime, datetime]:
    now = datetime.now()
    start_time, end_time = None, now  # Default end_time is now

    # ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç† "æœ€è¿‘Xåˆ†é’Ÿ/å°æ—¶/å¤©" æˆ– "è¿‡å»Xåˆ†é’Ÿ/å°æ—¶/å¤©"
    # æ”¯æŒæ•°å­—å’Œéƒ¨åˆ†ä¸­æ–‡æ•°å­—ï¼ˆä¸€è‡³åï¼‰
    num_map_chinese_to_int = {"ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5, "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9, "å": 10}
    # åŒ¹é…ä¾‹å¦‚: "è¿‡å»5åˆ†é’Ÿ", "æœ€è¿‘ä¸€å°æ—¶", "è¿‡å»åå¤©"
    match_duration = re.search(r"(?:æœ€è¿‘|è¿‡å»)([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*(åˆ†é’Ÿ|å°æ—¶|å¤©|å‘¨|æœˆ)", query_text)

    if match_duration:
        value_str = match_duration.group(1)
        unit = match_duration.group(2)
        value = 0

        if value_str in num_map_chinese_to_int:
            value = num_map_chinese_to_int[value_str]
        else:
            try:
                value = int(value_str)
            except ValueError:
                logging.warning(f"æ— æ³•ä» '{value_str}' è§£ææ•°å­—ç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢ã€‚")
                value = 0 
        
        if value > 0:
            if unit == "åˆ†é’Ÿ":
                start_time = now - timedelta(minutes=value)
            elif unit == "å°æ—¶":
                start_time = now - timedelta(hours=value)
            elif unit == "å¤©":
                start_time = now - timedelta(days=value)
            elif unit == "å‘¨":
                start_time = now - timedelta(weeks=value)
            elif unit == "æœˆ": # è¿‘ä¼¼æœˆä»½
                start_time = now - timedelta(days=value * 30)
            logging.info(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£ææ—¶é—´: å€¼={value}, å•ä½='{unit}'. è®¡ç®—å¼€å§‹æ—¶é—´: {start_time.isoformat() if start_time else 'N/A'}")

    # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ²¡æœ‰åŒ¹é…æˆåŠŸï¼Œå°è¯•ä½¿ç”¨ dateparser
    if start_time is None:
        parsed_dates = search_dates(
            query_text,
            languages=['zh'],
            settings={'PREFER_DATES_FROM': 'past', 'RETURN_AS_TIMEZONE_AWARE': False, 'RELATIVE_BASE': now}
        )
        logging.info(f"Dateparser ä¸ºæŸ¥è¯¢ '{query_text}' è¿”å›çš„ç»“æœ: {parsed_dates}")

        if parsed_dates:
            # å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„æ—¥æœŸå…³é”®è¯
            query_lower = query_text.lower()
            
            # å¤„ç†"å‰å¤©"
            if "å‰å¤©" in query_text:
                day_before_yesterday = now.date() - timedelta(days=2)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æ®µé™å®š
                if "ä¸Šåˆ" in query_text:
                    start_time = datetime.combine(day_before_yesterday, datetime.min.time().replace(hour=6))  # 6:00
                    end_time = datetime.combine(day_before_yesterday, datetime.min.time().replace(hour=12))   # 12:00
                    logging.info(f"æ£€æµ‹åˆ° 'å‰å¤©ä¸Šåˆ'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                elif "ä¸‹åˆ" in query_text:
                    start_time = datetime.combine(day_before_yesterday, datetime.min.time().replace(hour=12)) # 12:00
                    end_time = datetime.combine(day_before_yesterday, datetime.min.time().replace(hour=18))   # 18:00
                    logging.info(f"æ£€æµ‹åˆ° 'å‰å¤©ä¸‹åˆ'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                elif "æ™šä¸Š" in query_text or "å¤œé‡Œ" in query_text:
                    start_time = datetime.combine(day_before_yesterday, datetime.min.time().replace(hour=18)) # 18:00
                    end_time = datetime.combine(day_before_yesterday, datetime.max.time())                    # 23:59:59
                    logging.info(f"æ£€æµ‹åˆ° 'å‰å¤©æ™šä¸Š'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                else:
                    # æ•´ä¸ªå‰å¤©
                    start_time = datetime.combine(day_before_yesterday, datetime.min.time())
                    end_time = datetime.combine(day_before_yesterday, datetime.max.time())
                    logging.info(f"æ£€æµ‹åˆ° 'å‰å¤©'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
            
            # å¤„ç†"æ˜¨å¤©"
            elif any(pd[0] == "æ˜¨å¤©" for pd in parsed_dates) or "æ˜¨å¤©" in query_text:
                yesterday_date = now.date() - timedelta(days=1)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æ®µé™å®š
                if "ä¸Šåˆ" in query_text:
                    start_time = datetime.combine(yesterday_date, datetime.min.time().replace(hour=6))
                    end_time = datetime.combine(yesterday_date, datetime.min.time().replace(hour=12))
                    logging.info(f"æ£€æµ‹åˆ° 'æ˜¨å¤©ä¸Šåˆ'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                elif "ä¸‹åˆ" in query_text:
                    start_time = datetime.combine(yesterday_date, datetime.min.time().replace(hour=12))
                    end_time = datetime.combine(yesterday_date, datetime.min.time().replace(hour=18))
                    logging.info(f"æ£€æµ‹åˆ° 'æ˜¨å¤©ä¸‹åˆ'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                elif "æ™šä¸Š" in query_text or "å¤œé‡Œ" in query_text:
                    start_time = datetime.combine(yesterday_date, datetime.min.time().replace(hour=18))
                    end_time = datetime.combine(yesterday_date, datetime.max.time())
                    logging.info(f"æ£€æµ‹åˆ° 'æ˜¨å¤©æ™šä¸Š'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                else:
                    # æ•´ä¸ªæ˜¨å¤©
                    start_time = datetime.combine(yesterday_date, datetime.min.time())
                    end_time = datetime.combine(yesterday_date, datetime.max.time())
                    logging.info(f"æ£€æµ‹åˆ° 'æ˜¨å¤©'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
            
            # å¤„ç†"ä»Šå¤©"
            elif any(pd[0] == "ä»Šå¤©" for pd in parsed_dates) or "ä»Šå¤©" in query_text:
                today_date = now.date()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æ®µé™å®š
                if "ä¸Šåˆ" in query_text:
                    start_time = datetime.combine(today_date, datetime.min.time().replace(hour=6))
                    end_time = min(datetime.combine(today_date, datetime.min.time().replace(hour=12)), now)
                    logging.info(f"æ£€æµ‹åˆ° 'ä»Šå¤©ä¸Šåˆ'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                elif "ä¸‹åˆ" in query_text:
                    start_time = datetime.combine(today_date, datetime.min.time().replace(hour=12))
                    end_time = min(datetime.combine(today_date, datetime.min.time().replace(hour=18)), now)
                    logging.info(f"æ£€æµ‹åˆ° 'ä»Šå¤©ä¸‹åˆ'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                elif "æ™šä¸Š" in query_text or "å¤œé‡Œ" in query_text:
                    start_time = datetime.combine(today_date, datetime.min.time().replace(hour=18))
                    end_time = now  # åˆ°ç°åœ¨ä¸ºæ­¢
                    logging.info(f"æ£€æµ‹åˆ° 'ä»Šå¤©æ™šä¸Š'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
                else:
                    # ä»Šå¤©ä»å¼€å§‹åˆ°ç°åœ¨
                    start_time = datetime.combine(today_date, datetime.min.time())
                    end_time = now
                    logging.info(f"æ£€æµ‹åˆ° 'ä»Šå¤©'. æ—¶é—´èŒƒå›´: {start_time.isoformat()} åˆ° {end_time.isoformat()}")
            
            else:
                # å¯¹äºå…¶ä»– dateparser çš„ç»“æœï¼Œå–æœ€æ—©çš„è¿‡å»æ—¶é—´ç‚¹ä½œä¸ºå¼€å§‹æ—¶é—´
                # è¿™å¯èƒ½ä¸å®Œå…¨ç¬¦åˆç”¨æˆ·çš„æ„å›¾ï¼Œä½†ä½œä¸ºä¸€ç§å›é€€æœºåˆ¶
                potential_start_times = sorted([pd[1] for pd in parsed_dates if pd[1] < now])
                if potential_start_times:
                    start_time = potential_start_times[0]
                    logging.info(f"Dateparser é€šç”¨å›é€€æœºåˆ¶. ä½¿ç”¨æœ€æ—©çš„è§£ææ—¶é—´ç‚¹ä½œä¸ºå¼€å§‹æ—¶é—´: {start_time.isoformat()}")
    
    # å¦‚æœä»¥ä¸Šæ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½ç¡®å®š start_timeï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ—¶é—´çª—å£
    if start_time is None:
        start_time = now - timedelta(minutes=default_minutes_ago)
        logging.info(f"æœªèƒ½ä»æŸ¥è¯¢ä¸­è§£æç‰¹å®šæ—¶é—´ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´çª—å£: {default_minutes_ago} åˆ†é’Ÿå‰. å¼€å§‹æ—¶é—´: {start_time.isoformat()}")

    # ç¡®ä¿ start_time ä¸æ™šäº end_time
    if start_time > end_time:
        logging.warning(f"è§£æåçš„å¼€å§‹æ—¶é—´ ({start_time.isoformat()}) æ™šäºç»“æŸæ—¶é—´ ({end_time.isoformat()}). å°†è°ƒæ•´å¼€å§‹æ—¶é—´ã€‚")
        start_time = end_time - timedelta(seconds=1) # åˆ›å»ºä¸€ä¸ªæå°ä½†æœ‰æ•ˆçš„æ—¶é—´èŒƒå›´

    return start_time, end_time


def get_all_activity_records(limit: int = 50) -> list:
    """
    ä»SQLiteæ•°æ®åº“ä¸­æ£€ç´¢æ´»åŠ¨è®°å½•ã€‚
    é»˜è®¤æŒ‰æ—¶é—´æˆ³é™åºæ’åºï¼Œè¿”å›æœ€æ–°çš„è®°å½•ã€‚
    """
    conn = create_db_connection()
    if not conn:
        logging.error("get_all_activity_records: æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ã€‚")
        return []

    records = []
    try:
        cursor = conn.cursor()
        # è·å–æœ€æ–°çš„è®°å½•
        cursor.execute("SELECT * FROM activity_log ORDER BY timestamp DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        for row in rows:
            records.append(dict(row))  # å°† sqlite3.Row å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        logging.info(f"æˆåŠŸä»æ•°æ®åº“æ£€ç´¢åˆ° {len(records)} æ¡æ´»åŠ¨è®°å½• (é™åˆ¶: {limit}).")
    except sqlite3.Error as e:
        logging.error(f"ä»æ•°æ®åº“æ£€ç´¢æ´»åŠ¨è®°å½•å¤±è´¥: {e}")
        records = [] # ç¡®ä¿å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨
    finally:
        if conn:
            conn.close()
    return records

async def get_application_usage_summary(start_time_dt: datetime, end_time_dt: datetime) -> dict:
    """
    è®¡ç®—åœ¨ç»™å®šæ—¶é—´èŒƒå›´å†…æ¯ä¸ªåº”ç”¨ç¨‹åºçš„ä½¿ç”¨æ—¶é•¿ã€‚
    """
    conn = create_db_connection()
    if not conn:
        return {"error": "æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ä»¥è®¡ç®—åº”ç”¨ä½¿ç”¨æ—¶é•¿ã€‚"}

    usage_summary = defaultdict(timedelta)
    # ç”¨äºè°ƒè¯•çš„åŸå§‹äº‹ä»¶æ ·æœ¬
    raw_events_for_period = [] 

    try:
        cursor = conn.cursor()
        
        # å°† datetime å¯¹è±¡è½¬æ¢ä¸º ISO æ ¼å¼çš„å­—ç¬¦ä¸²ä»¥è¿›è¡Œ SQL æŸ¥è¯¢
        start_time_iso = start_time_dt.isoformat()
        end_time_iso = end_time_dt.isoformat()

        # æŸ¥è¯¢æ­¤æ—¶é—´æ®µå†…æ‰€æœ‰çš„ 'app_switch' å’Œ 'screen_content' äº‹ä»¶ï¼ŒæŒ‰æ—¶é—´æˆ³æ’åº
        # æˆ‘ä»¬éœ€è¦app_nameå’Œtimestamp
        # 'mouse_interaction' é€šå¸¸ä¹Ÿå‘ç”Ÿåœ¨æŸä¸ªåº”ç”¨å†…ï¼Œä½†å¦‚æœåªå…³å¿ƒåº”ç”¨åˆ‡æ¢å’Œä¸»è¦å†…å®¹ï¼Œå¯ä»¥å…ˆå…³æ³¨å‰ä¸¤è€…
        # ä¸ºäº†æ›´å‡†ç¡®ï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘æ‰€æœ‰å¸¦æœ‰ app_name çš„è®°å½•ç±»å‹
        query = """
            SELECT timestamp, app_name, record_type, url
            FROM activity_log
            WHERE timestamp >= ? AND timestamp <= ? AND app_name IS NOT NULL AND app_name != 'Unknown'
            ORDER BY timestamp ASC
        """
        cursor.execute(query, (start_time_iso, end_time_iso))
        events = cursor.fetchall()
        
        # logging.debug(f"è·å–åˆ° {len(events)} æ¡äº‹ä»¶ç”¨äºè®¡ç®— {start_time_iso} åˆ° {end_time_iso} ä¹‹é—´çš„åº”ç”¨æ—¶é•¿ã€‚")
        # for event_row in events[:5]: # æ‰“å°ä¸€äº›æ ·æœ¬äº‹ä»¶
        #     logging.debug(f"  äº‹ä»¶æ ·æœ¬: {dict(event_row)}")


        if not events:
            return {"usage": {}, "raw_events": [], "message": "æŒ‡å®šæ—¶é—´æ®µå†…æ— ç›¸å…³æ´»åŠ¨è®°å½•ã€‚"}

        # å°†åŸå§‹äº‹ä»¶æ·»åŠ åˆ°è°ƒè¯•è¾“å‡º
        for event_row in events:
            raw_events_for_period.append(dict(event_row))


        # è®¡ç®—é€»è¾‘:
        # éå†æ’åºåçš„äº‹ä»¶ã€‚å¯¹äºæ¯ä¸ªäº‹ä»¶ï¼Œå®ƒä»£è¡¨äº†ä»è¯¥äº‹ä»¶æ—¶é—´ç‚¹åˆ°ä¸‹ä¸€ä¸ªä¸åŒåº”ç”¨äº‹ä»¶æ—¶é—´ç‚¹ä¹‹é—´ï¼Œ
        # å½“å‰åº”ç”¨æ˜¯æ´»åŠ¨çŠ¶æ€çš„ã€‚
        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªäº‹ä»¶ï¼Œåˆ™å®ƒä»£è¡¨ä»è¯¥äº‹ä»¶æ—¶é—´ç‚¹åˆ°æŸ¥è¯¢èŒƒå›´çš„ç»“æŸæ—¶é—´ç‚¹ã€‚

        for i in range(len(events)):
            current_event_dict = dict(events[i])
            current_app = current_event_dict['app_name']
            
            try:
                # sqlite3.Row['timestamp'] è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                current_event_time = datetime.fromisoformat(current_event_dict['timestamp'])
            except ValueError:
                logging.warning(f"æ— æ³•è§£æäº‹ä»¶æ—¶é—´æˆ³: {current_event_dict['timestamp']} for app {current_app}. è·³è¿‡æ­¤äº‹ä»¶å¯¹ã€‚")
                continue

            if not current_app or current_app == "Unknown": # å†æ¬¡ç¡®è®¤ï¼Œè™½ç„¶æŸ¥è¯¢æ—¶å·²è¿‡æ»¤
                continue
            
            next_event_time = end_time_dt # é»˜è®¤åˆ°æŸ¥è¯¢èŒƒå›´çš„ç»“æŸ

            if i + 1 < len(events):
                next_event_dict = dict(events[i+1])
                try:
                    next_event_time_candidate = datetime.fromisoformat(next_event_dict['timestamp'])
                    # å¦‚æœä¸‹ä¸€ä¸ªäº‹ä»¶çš„æ—¶é—´è¶…å‡ºäº†æŸ¥è¯¢èŒƒå›´çš„ç»“æŸæ—¶é—´ï¼Œåˆ™æˆªæ–­åˆ°æŸ¥è¯¢ç»“æŸæ—¶é—´
                    next_event_time = min(next_event_time_candidate, end_time_dt)
                except ValueError:
                    logging.warning(f"æ— æ³•è§£æä¸‹ä¸€ä¸ªäº‹ä»¶æ—¶é—´æˆ³: {next_event_dict['timestamp']}. å°†ä½¿ç”¨èŒƒå›´ç»“æŸæ—¶é—´ã€‚")
                    # next_event_time ä¿æŒä¸º end_time_dt
            
            # ç¡®ä¿ current_event_time ä¸æ™šäº next_event_time (ä¾‹å¦‚ï¼Œå¦‚æœæ•°æ®æœ‰è¯¯æˆ–éƒ½åœ¨åŒä¸€ç§’)
            if current_event_time < next_event_time:
                duration = next_event_time - current_event_time
                usage_summary[current_app] += duration
            elif current_event_time == next_event_time and i == len(events) -1 :
                # å¦‚æœæ˜¯æœ€åä¸€ä¸ªäº‹ä»¶ä¸”æ—¶é—´ä¸end_time_dtç›¸åŒï¼Œç»™ä¸€ä¸ªè±¡å¾æ€§çš„çŸ­æ—¶é—´ï¼ˆä¾‹å¦‚1ç§’ï¼‰
                # æˆ–è€…åŸºäºå®ƒæ˜¯æœ€åä¸€ä¸ªäº‹ä»¶çš„ä¸Šä¸‹æ–‡æ¥å†³å®šã€‚
                # è¿™é‡Œç®€åŒ–ï¼Œå¦‚æœæ—¶é—´ç›¸åŒï¼Œè®¤ä¸ºæŒç»­æ—¶é—´ä¸º0ï¼Œé™¤éæœ‰ç‰¹æ®Šå¤„ç†éœ€æ±‚ã€‚
                pass


        # logging.debug(f"è®¡ç®—åçš„åº”ç”¨æ—¶é•¿ (åŸå§‹): {dict(usage_summary)}")
        
        # è¿”å›ç»“æœï¼ŒåŒ…å«æ€»æ—¶é•¿å’ŒåŸå§‹äº‹ä»¶æ ·æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        return {
            "usage": dict(usage_summary), # å°†defaultdictè½¬ä¸ºæ™®é€šdict
            "raw_events": raw_events_for_period # è¿”å›åœ¨æ­¤æœŸé—´å¤„ç†çš„æ‰€æœ‰äº‹ä»¶
        }

    except sqlite3.Error as e:
        logging.error(f"è®¡ç®—åº”ç”¨ä½¿ç”¨æ—¶é•¿æ—¶æ•°æ®åº“æŸ¥è¯¢å‡ºé”™: {e}")
        return {"error": f"æ•°æ®åº“é”™è¯¯: {e}", "usage": {}, "raw_events": []}
    except Exception as e_global:
        logging.error(f"è®¡ç®—åº”ç”¨ä½¿ç”¨æ—¶é•¿æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e_global}", exc_info=True)
        return {"error": f"æœªçŸ¥é”™è¯¯: {e_global}", "usage": {}, "raw_events": []}
    finally:
        if conn:
            conn.close()

# æ³¨é‡Šæ‰æ•´ä¸ª __main__ å—ä»¥é¿å…å¯¼å…¥æ—¶æ‰§è¡Œ
# if __name__ == "__main__":
#     # ä¸ºäº†è¿è¡Œå¼‚æ­¥çš„ main_test_query
#     # asyncio.run(main_test_query())

#     # é¦–å…ˆï¼Œç¡®ä¿æ•°æ®è¢«åŠ è½½å’Œç´¢å¼•
#     # è¿™ä¸€æ­¥åªéœ€è¦åšä¸€æ¬¡ï¼Œæˆ–è€…åœ¨æ•°æ®æ›´æ–°æ—¶å®šæœŸåš
#     # å¯¹äºæµ‹è¯•ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯æ¬¡è¿è¡Œæ—¶éƒ½åŠ è½½æœ€æ–°çš„æ•°æ®
#     print("æ­£åœ¨åŠ è½½å’Œç´¢å¼•å±å¹•æ´»åŠ¨æ•°æ®...")
#     count = load_and_index_activity_data()
#     print(f"åŠ è½½äº† {count} æ¡æ–°è®°å½•ã€‚")

#     if activity_vector_store is not None: # ç¡®ä¿æ•°æ®åº“å¯¹è±¡ä¸æ˜¯Noneæ‰å°è¯•æŸ¥è¯¢
#         # å¹¶ä¸”æœ€å¥½ä¹Ÿæ£€æŸ¥ä¸€ä¸‹æ•°æ®åº“ä¸­æ˜¯å¦æœ‰æ•°æ®è¢«åŠ è½½è¿›å»
#         # è¿™ä¸ªæ£€æŸ¥å¯ä»¥æ”¾åœ¨ main_test_query å†…éƒ¨æˆ–è¿™é‡Œ
#         db_is_empty = True
#         try:
#             # å°è¯•è·å–é›†åˆä¸­çš„æ¡ç›®æ•°é‡ï¼Œå¦‚æœä¸º0ï¼Œåˆ™è®¤ä¸ºç©º
#             # æ³¨æ„: .count() æ˜¯æ–°ç‰ˆchromadbçš„æ–¹æ³•ï¼Œlangchainçš„Chromaå¯èƒ½æ²¡æœ‰ç›´æ¥çš„ .count()
#             # æˆ‘ä»¬é€šè¿‡ .get() æ¥é—´æ¥åˆ¤æ–­
#             retrieved_items = activity_vector_store.get(limit=1) 
#             if retrieved_items and retrieved_items.get('ids') and len(retrieved_items.get('ids')) > 0:
#                 db_is_empty = False
#             elif count > 0 and (not retrieved_items or not retrieved_items.get('ids')):
#                 logging.warning("æ•°æ®å·²å°è¯•åŠ è½½åˆ°Chromaï¼Œä½† .get(limit=1) æœªè¿”å›æœ‰æ•ˆIDï¼Œå¯èƒ½æŒä¹…åŒ–æˆ–é›†åˆå†…éƒ¨æœ‰é—®é¢˜ã€‚")
#         except Exception as e:
#             logging.error(f"æ£€æŸ¥æ•°æ®åº“æ˜¯å¦ä¸ºç©ºæ—¶å‡ºé”™: {e}")

#         if count > 0 and not db_is_empty:
#             import asyncio
#             # æµ‹è¯•åº”ç”¨æ—¶é•¿è®¡ç®—
#             async def test_usage_calculation():
#                 # await main_test_query()  # æ³¨é‡Šæ‰æœªå®šä¹‰çš„å‡½æ•°è°ƒç”¨
#                 print("\n--- æµ‹è¯•åº”ç”¨æ—¶é•¿è®¡ç®— (è¿‡å»60åˆ†é’Ÿ) ---")
#                 now = datetime.now()
#                 start_calc_time = now - timedelta(minutes=60)
#                 usage_summary = await get_application_usage_summary(start_calc_time, now)
#                 if usage_summary.get("error"):
#                     print(f"è®¡ç®—å‡ºé”™: {usage_summary['error']}")
#                 else:
#                     print("åº”ç”¨ä½¿ç”¨æ—¶é•¿:")
#                     for app, duration in usage_summary["usage"].items():
#                         print(f"  {app}: {duration}")
#                 # print("\nåŸå§‹äº‹ä»¶:")
#                 # for evt in usage_summary["raw_events"][:10]: # æ‰“å°å‰10æ¡äº‹ä»¶è°ƒè¯•
#                 #     print(evt)
#             asyncio.run(test_usage_calculation())
#         elif count == 0 and db_is_empty:
#             print(f"æ²¡æœ‰æ–°çš„æ´»åŠ¨è®°å½•è¢«åŠ è½½ï¼Œä¸”æ•°æ®åº“ ({CHROMA_COLLECTION_NAME_ACTIVITY}) ä¸ºç©ºã€‚è¯·ç¡®ä¿ screen_capture.py å·²è¿è¡Œå¹¶ç”Ÿæˆæ•°æ®ã€‚")
#         elif count > 0 and db_is_empty:
#             print(f"{count} æ¡è®°å½•å°è¯•åŠ è½½ï¼Œä½†æ•°æ®åº“ ({CHROMA_COLLECTION_NAME_ACTIVITY}) ä¼¼ä¹ä»ä¸ºç©ºæˆ–æ— æ³•æ­£ç¡®è¯»å–ã€‚è¯·æ£€æŸ¥ChromaDBçš„æŒä¹…åŒ–å’ŒæŸ¥è¯¢ã€‚")
#         else: # activity_vector_store is None çš„æƒ…å†µ
#             print(f"å‘é‡æ•°æ®åº“æœªèƒ½æˆåŠŸåˆå§‹åŒ–ã€‚æ— æ³•è¿›è¡ŒæŸ¥è¯¢ã€‚")
#     else:
#         print(f"å‘é‡æ•°æ®åº“æœªèƒ½æˆåŠŸåˆå§‹åŒ– (activity_vector_store is None)ã€‚æ— æ³•è¿›è¡ŒæŸ¥è¯¢ã€‚")