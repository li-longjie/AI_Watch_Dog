import httpx
import requests
from typing import Dict, Any
from datetime import datetime
import traceback
from .base_tool import BaseMCPTool

class SequentialThinkingTool(BaseMCPTool):
    """Sequential Thinkingæ¨ç†å·¥å…·"""
    
    @property
    def tool_name(self) -> str:
        return "Sequential Thinkingæ¨ç†å·¥å…·"
    
    @property
    def description(self) -> str:
        return "ç”¨äºå¤æ‚é—®é¢˜çš„é€æ­¥æ€è€ƒå’Œæ¨ç†ï¼Œå¸®åŠ©å¤§æ¨¡å‹è¿›è¡Œç»“æ„åŒ–åˆ†æ"
    
    def get_available_functions(self) -> Dict[str, Dict[str, Any]]:
        return {
            "sequential_thinking": {
                "description": "å¯¹å¤æ‚é—®é¢˜è¿›è¡Œé€æ­¥æ€è€ƒå’Œæ¨ç†",
                "parameters": {
                    "prompt": {
                        "type": "string", 
                        "description": "éœ€è¦åˆ†æçš„é—®é¢˜æˆ–ä»»åŠ¡",
                        "required": True
                    },
                    "max_steps": {
                        "type": "integer",
                        "description": "æœ€å¤§æ€è€ƒæ­¥éª¤æ•°ï¼Œé»˜è®¤5æ­¥",
                        "required": False
                    }
                },
                "examples": [
                    "å¸®æˆ‘åˆ†æè¿™ä¸ªå¤æ‚çš„æ•°å­¦é—®é¢˜",
                    "é€æ­¥æ€è€ƒè¿™ä¸ªå•†ä¸šå†³ç­–",
                    "åˆ†æè¿™ä¸ªç¼–ç¨‹é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ",
                    "é€æ­¥æ¨ç†è¿™ä¸ªé€»è¾‘é—®é¢˜"
                ]
            }
        }
    
    async def execute_function(self, function_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒSequential ThinkingåŠŸèƒ½"""
        if function_name == "sequential_thinking":
            return await self._sequential_thinking(parameters)
        else:
            return {
                "status": "error",
                "message": f"æœªçŸ¥å‡½æ•°: {function_name}"
            }
    
    async def _sequential_thinking(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒSequential Thinkingæ¨ç†"""
        try:
            prompt = parameters.get("prompt")
            max_steps = parameters.get("max_steps", 5)
            
            if not prompt:
                return {
                    "status": "error",
                    "message": "ç¼ºå°‘å¿…éœ€å‚æ•°: prompt"
                }
            
            # è°ƒç”¨Sequential Thinkingä¸­é—´ä»¶
            result = await self._call_sequential_thinking_with_llm(prompt, max_steps)
            
            if "error" in result:
                return {
                    "status": "error",
                    "message": result["error"]
                }
            
            return {
                "status": "success",
                "thinking_result": result,
                "formatted_response": self._format_middleware_result(result)
            }
            
        except Exception as e:
            self.logger.error(f"Sequential Thinkingæ‰§è¡Œé”™è¯¯: {e}")
            return {
                "status": "error",
                "message": f"Sequential Thinkingæ‰§è¡Œå¤±è´¥: {str(e)}"
            }
    
    async def _call_sequential_thinking_with_llm(self, prompt: str, max_steps: int = 5) -> Dict[str, Any]:
        """ä½¿ç”¨Sequential Thinkingä½œä¸ºä¸­é—´ä»¶ååŠ©å¤§æ¨¡å‹æ¨ç†"""
        try:
            self.logger.info(f"å¯åŠ¨Sequential Thinkingè¾…åŠ©æ¨ç†: {prompt[:100]}...")
            
            st_endpoint = f"{self.base_url}/sequential-thinking/sequentialthinking"
            thinking_results = []
            
            # ä¿å­˜å®Œæ•´çš„åŸå§‹é—®é¢˜
            original_question = prompt
            
            # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨Sequential Thinkingè§„åˆ’æ€è€ƒæ­¥éª¤
            self.logger.info("=== é˜¶æ®µ1ï¼šSequential Thinkingè§„åˆ’æ€è€ƒæµç¨‹ ===")
            
            planning_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é€»è¾‘æ¨ç†ä¸“å®¶ï¼Œéœ€è¦å¯¹ä»¥ä¸‹å¤æ‚é—®é¢˜è¿›è¡Œç»“æ„åŒ–åˆ†æã€‚è¯·è®¾è®¡{max_steps}ä¸ªé€æ­¥æ¨ç†çš„æ­¥éª¤ï¼Œä½“ç°ä¸¥å¯†çš„é€»è¾‘æ€ç»´ï¼š

å®Œæ•´é—®é¢˜ï¼š{original_question}

è¯·æŒ‰ç…§ä»¥ä¸‹åŸåˆ™è®¾è®¡æ¨ç†æ­¥éª¤ï¼š
1. **å‡è®¾åˆ†ææ³•**ï¼šå¯¹æ¯ä¸ªå¯èƒ½æ€§è¿›è¡Œç³»ç»Ÿæ€§å‡è®¾å’ŒéªŒè¯
2. **çŸ›ç›¾æ’é™¤æ³•**ï¼šå¯»æ‰¾é€»è¾‘çŸ›ç›¾ï¼Œæ’é™¤ä¸å¯èƒ½çš„é€‰é¡¹
3. **æ¡ä»¶æ¢³ç†**ï¼šæ˜ç¡®æ‰€æœ‰çº¦æŸæ¡ä»¶å’Œå·²çŸ¥ä¿¡æ¯
4. **é€æ­¥æ¨è¿›**ï¼šæ¯ä¸ªæ­¥éª¤éƒ½åŸºäºå‰ä¸€æ­¥çš„ç»“è®º
5. **åè¯éªŒè¯**ï¼šç”¨åè¯æ³•éªŒè¯æœ€ç»ˆç»“è®º

æ ¼å¼è¦æ±‚ï¼š
æ­¥éª¤1ï¼š[æ˜ç¡®ä¿¡æ¯æ¢³ç†] - [åˆ—å‡ºæ‰€æœ‰å·²çŸ¥æ¡ä»¶ã€çº¦æŸå’Œé™ˆè¿°ï¼Œå»ºç«‹é€»è¾‘æ¡†æ¶] - [å®Œæ•´çš„æ¡ä»¶æ¸…å•å’Œé€»è¾‘å…³ç³»å›¾]
æ­¥éª¤2ï¼š[å‡è®¾Açš„æƒ…å†µ] - [å‡è®¾æŸä¸ªå…³é”®æ¡ä»¶ä¸ºçœŸï¼Œæ¨å¯¼æ‰€æœ‰åæœ] - [è¯¥å‡è®¾ä¸‹çš„å®Œæ•´æ¨ç†é“¾å’Œç»“è®º]
æ­¥éª¤3ï¼š[å‡è®¾Bçš„æƒ…å†µ] - [å‡è®¾å¦ä¸€ä¸ªå…³é”®æ¡ä»¶ä¸ºçœŸï¼Œæ¨å¯¼æ‰€æœ‰åæœ] - [è¯¥å‡è®¾ä¸‹çš„å®Œæ•´æ¨ç†é“¾å’Œç»“è®º]
æ­¥éª¤4ï¼š[çŸ›ç›¾æ£€éªŒä¸æ’é™¤] - [æ£€æŸ¥å„å‡è®¾æ˜¯å¦äº§ç”Ÿé€»è¾‘çŸ›ç›¾ï¼Œæ’é™¤ä¸å¯èƒ½æƒ…å†µ] - [æœ‰æ•ˆå‡è®¾åˆ—è¡¨å’Œæ’é™¤ç†ç”±]
æ­¥éª¤5ï¼š[æœ€ç»ˆæ¨è®ºä¸éªŒè¯] - [åŸºäºæ’é™¤æ³•ç¡®å®šå”¯ä¸€è§£ï¼Œå¹¶è¿›è¡Œåè¯éªŒè¯] - [æœ€ç»ˆç­”æ¡ˆå’Œå®Œæ•´è¯æ˜è¿‡ç¨‹]
"""
            
            # è·å–æ€è€ƒè§„åˆ’ (è¿™é‡Œéœ€è¦è°ƒç”¨LLMæœåŠ¡)
            planning_result = await self._query_llm_model(planning_prompt)
            self.logger.info(f"æ€è€ƒè§„åˆ’å®Œæˆ: {planning_result[:200]}...")
            
            # è§£æè§„åˆ’ç»“æœ
            steps_plan = self._parse_thinking_plan(planning_result, max_steps)
            
            # ç¬¬äºŒé˜¶æ®µï¼šSequential ThinkingçŠ¶æ€ç®¡ç† + å¤§æ¨¡å‹æ‰§è¡Œ
            self.logger.info("=== é˜¶æ®µ2ï¼šæ‰§è¡Œç»“æ„åŒ–æ€è€ƒ ===")
            
            for step_num in range(1, max_steps + 1):
                # 2.1 å‘Sequential Thinkingæ³¨å†Œå½“å‰æ­¥éª¤
                st_request = {
                    "thought": f"æ‰§è¡Œæ­¥éª¤{step_num}ï¼š{steps_plan.get(step_num, {}).get('title', f'ç¬¬{step_num}æ­¥æ€è€ƒ')}",
                    "nextThoughtNeeded": step_num < max_steps,
                    "thoughtNumber": step_num,
                    "totalThoughts": max_steps,
                    "needsMoreThoughts": step_num < max_steps
                }
                
                try:
                    async with httpx.AsyncClient(timeout=60.0) as client:
                        st_response = await client.post(st_endpoint, json=st_request)
                        
                        if st_response.status_code == 200:
                            st_result = st_response.json()
                            self.logger.info(f"Sequential Thinkingæ­¥éª¤{step_num}æ³¨å†ŒæˆåŠŸï¼Œå†å²é•¿åº¦: {st_result.get('thoughtHistoryLength')}")
                            
                            # 2.2 ä½¿ç”¨å¤§æ¨¡å‹æ‰§è¡Œå…·ä½“æ€è€ƒ
                            step_info = steps_plan.get(step_num, {})
                            
                            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡
                            context = f"""
ğŸ§  **é€»è¾‘æ¨ç†ä¸“å®¶æ¨¡å¼** - ç¬¬{step_num}æ­¥æ¨ç†

ã€åŸå§‹é—®é¢˜ã€‘
{original_question}

ã€å½“å‰ä»»åŠ¡ã€‘
æ­¥éª¤{step_num}ï¼š{step_info.get('title', f'ç¬¬{step_num}æ­¥')}
æ¨ç†è¦æ±‚ï¼š{step_info.get('content', 'ç»§ç»­åˆ†æé—®é¢˜')}
é¢„æœŸæˆæœï¼š{step_info.get('expected', 'åˆ†æç»“æœ')}

ã€å‰ç½®æ¨ç†é“¾ã€‘
{self._format_complete_previous_results(thinking_results) if thinking_results else ''}

ã€æ¨ç†æŒ‡å¯¼åŸåˆ™ã€‘
1. **ä¸¥æ ¼é€»è¾‘**ï¼šæ¯ä¸ªæ¨è®ºéƒ½è¦æœ‰æ˜ç¡®çš„é€»è¾‘ä¾æ®
2. **å‡è®¾éªŒè¯**ï¼šæ˜ç¡®åŒºåˆ†å‡è®¾å’Œå·²çŸ¥äº‹å®
3. **çŸ›ç›¾æ£€æµ‹**ï¼šä¸»åŠ¨å¯»æ‰¾å’Œè¯†åˆ«é€»è¾‘çŸ›ç›¾
4. **å®Œæ•´æ€§æ£€æŸ¥**ï¼šç¡®ä¿è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„æƒ…å†µ
5. **æ¨ç†å¯è§†åŒ–**ï¼šç”¨ç¬¦å·åŒ–è¡¨ç¤ºå¤æ‚çš„é€»è¾‘å…³ç³»

è¯·ä»¥ä¸“ä¸šé€»è¾‘æ¨ç†ä¸“å®¶çš„èº«ä»½ï¼Œè¿›è¡Œä¸¥å¯†çš„åˆ†æã€‚

**é‡è¦è¦æ±‚ï¼šæœ¬æ­¥éª¤éœ€è¦æœ‰æ¸…æ™°çš„æ¨ç†è¿‡ç¨‹ï¼Œä½†ä¿æŒç®€æ´ï¼ˆ100-150å­—ï¼‰**

åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼š
- æ˜ç¡®æ ‡å‡ºã€å‡è®¾ã€‘ã€ã€å·²çŸ¥ã€‘ã€ã€æ¨è®ºã€‘ã€ã€çŸ›ç›¾ã€‘
- ä½¿ç”¨é€»è¾‘ç¬¦å·ï¼ˆâ†’, âˆ§, âˆ¨, Â¬ï¼‰è¡¨ç¤ºå…³é”®å…³ç³»
- æä¾›æ ¸å¿ƒæ¨ç†ä¾æ®ï¼Œä¸è·³è¿‡å…³é”®æ­¥éª¤
- ç®€æ˜æ‰¼è¦åœ°è¯´æ˜æ¨ç†é€»è¾‘
- è¯†åˆ«çŸ›ç›¾å’Œå…³é”®è½¬æŠ˜ç‚¹

è¯·ç¡®ä¿è¿™ä¸€æ­¥æœ‰æ¸…æ™°çš„æ¨ç†é“¾æ¡ï¼Œä½†é¿å…å†—é•¿æè¿°ã€‚
"""
                            
                            step_result = await self._query_llm_model(context)
                            
                            # ä¿å­˜æ­¥éª¤ç»“æœ
                            thinking_results.append({
                                "step_number": step_num,
                                "title": step_info.get('title', f'ç¬¬{step_num}æ­¥'),
                                "content": step_result,
                                "st_state": st_result,
                                "timestamp": datetime.now().isoformat()
                            })
                            
                            self.logger.info(f"æ­¥éª¤{step_num}æ‰§è¡Œå®Œæˆ")
                            
                        else:
                            self.logger.warning(f"Sequential Thinkingæ­¥éª¤{step_num}æ³¨å†Œå¤±è´¥: {st_response.text}")
                            # å³ä½¿STå¤±è´¥ï¼Œä¹Ÿç»§ç»­ç”¨å¤§æ¨¡å‹æ‰§è¡Œ
                            step_info = steps_plan.get(step_num, {})
                            context = f"å®Œæ•´é—®é¢˜ï¼š{original_question}\nç¬¬{step_num}æ­¥ï¼š{step_info.get('content', 'ç»§ç»­åˆ†æ')}"
                            step_result = await self._query_llm_model(context)
                            
                            thinking_results.append({
                                "step_number": step_num,
                                "title": step_info.get('title', f'ç¬¬{step_num}æ­¥'),
                                "content": step_result,
                                "st_state": None,
                                "timestamp": datetime.now().isoformat()
                            })
                            
                except Exception as e:
                    self.logger.error(f"æ­¥éª¤{step_num}æ‰§è¡Œå¼‚å¸¸: {e}")
                    # å¤±è´¥æ—¶ä¹Ÿè¦ç»§ç»­
                    continue
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šç»¼åˆæ€»ç»“
            self.logger.info("=== é˜¶æ®µ3ï¼šç»¼åˆæ€»ç»“ ===")
            
            summary_prompt = f"""
ğŸ¯ **é€»è¾‘æ¨ç†æ€»ç»“ä¸éªŒè¯**

ã€åŸå§‹é—®é¢˜ã€‘
{original_question}

ã€å®Œæ•´æ¨ç†è¿‡ç¨‹ã€‘
{self._format_complete_thinking_results(thinking_results)}

ä½œä¸ºé€»è¾‘æ¨ç†ä¸“å®¶ï¼Œè¯·æä¾›ä¸“ä¸šçš„æ€»ç»“æŠ¥å‘Šï¼š

## ğŸ“‹ æ¨ç†è¿‡ç¨‹å›é¡¾
- æ€»ç»“æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ ¸å¿ƒé€»è¾‘
- å±•ç¤ºå‡è®¾â†’æ¨è®ºâ†’éªŒè¯çš„å®Œæ•´é“¾æ¡
- çªå‡ºå…³é”®çš„çŸ›ç›¾æ’é™¤è¿‡ç¨‹

## ğŸ” é€»è¾‘éªŒè¯æ£€æŸ¥
- éªŒè¯æ¨ç†é“¾çš„å®Œæ•´æ€§å’Œä¸¥å¯†æ€§
- æ£€æŸ¥æ˜¯å¦å­˜åœ¨é€»è¾‘æ¼æ´æˆ–é—æ¼
- ç¡®è®¤æ’é™¤æ³•çš„å½»åº•æ€§

## ğŸ’¡ æœ€ç»ˆç»“è®º
- æ˜ç¡®çš„ç­”æ¡ˆå’Œæ¨ç†ä¾æ®
- ä¸ºä»€ä¹ˆè¿™æ˜¯å”¯ä¸€æ­£ç¡®çš„è§£
- åè¯ï¼šä¸ºä»€ä¹ˆå…¶ä»–å¯èƒ½æ€§è¢«æ’é™¤

## ğŸ“ æ¨ç†æ–¹æ³•æ€»ç»“
- ä½¿ç”¨äº†å“ªäº›é€»è¾‘æ¨ç†æŠ€å·§
- ä½“ç°äº†Sequential Thinkingçš„å“ªäº›ä¼˜åŠ¿
- è¿™ç§æ–¹æ³•ç›¸æ¯”ç›´è§‰åˆ¤æ–­çš„ä¼˜åŠ¿

è¯·ç¡®ä¿æ€»ç»“ä½“ç°å‡ºä¸“ä¸šé€»è¾‘æ¨ç†å·¥å…·çš„ä»·å€¼å’Œä¸¥è°¨æ€§ã€‚
"""
            
            final_summary = await self._query_llm_model(summary_prompt)
            
            return {
                "original_prompt": original_question,
                "planning": planning_result,
                "steps": thinking_results,
                "summary": final_summary,
                "total_steps": len(thinking_results),
                "method": "sequential_thinking_middleware",
                "completed": True
            }
            
        except Exception as e:
            self.logger.error(f"Sequential Thinkingä¸­é—´ä»¶å¼‚å¸¸: {str(e)}")
            self.logger.error(traceback.format_exc())
            return {"error": f"Sequential Thinkingä¸­é—´ä»¶æ‰§è¡Œå¼‚å¸¸: {str(e)}"}
    
    async def _query_llm_model(self, prompt: str) -> str:
        """è°ƒç”¨LLMæ¨¡å‹ï¼ˆè¿™é‡Œéœ€è¦é›†æˆåˆ°ç°æœ‰çš„LLMæœåŠ¡ï¼‰"""
        try:
            # è¿™é‡Œéœ€è¦è°ƒç”¨ç°æœ‰çš„LLMæœåŠ¡
            # æš‚æ—¶è¿”å›ç®€åŒ–å“åº”ï¼Œç¨åä¼šé›†æˆåˆ°ä¸»ç³»ç»Ÿ
            from llm_service import chat_completion
            return await chat_completion(prompt)
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
    
    def _parse_thinking_plan(self, planning_text: str, max_steps: int) -> Dict[int, Dict[str, str]]:
        """è§£ææ€è€ƒè§„åˆ’"""
        steps = {}
        lines = planning_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('æ­¥éª¤') and 'ï¼š' in line:
                try:
                    # è§£æï¼šæ­¥éª¤1ï¼š[æ ‡é¢˜] - [å†…å®¹] - [é¢„æœŸ]
                    parts = line.split('ï¼š', 1)[1].split(' - ')
                    step_num = int(line.split('ï¼š')[0].replace('æ­¥éª¤', ''))
                    
                    if step_num <= max_steps:
                        steps[step_num] = {
                            'title': parts[0].strip() if len(parts) > 0 else f'ç¬¬{step_num}æ­¥',
                            'content': parts[1].strip() if len(parts) > 1 else 'ç»§ç»­åˆ†æ',
                            'expected': parts[2].strip() if len(parts) > 2 else 'åˆ†æç»“æœ'
                        }
                except:
                    continue
        
        # ç¡®ä¿æ‰€æœ‰æ­¥éª¤éƒ½æœ‰é»˜è®¤å€¼
        for i in range(1, max_steps + 1):
            if i not in steps:
                steps[i] = {
                    'title': f'ç¬¬{i}æ­¥æ€è€ƒ',
                    'content': 'ç»§ç»­åˆ†æé—®é¢˜',
                    'expected': 'åˆ†æç»“æœ'
                }
        
        return steps
    
    def _format_complete_previous_results(self, results: list) -> str:
        """æ ¼å¼åŒ–å‰é¢æ­¥éª¤çš„å®Œæ•´ç»“æœ"""
        if not results:
            return ""
        
        formatted = "å‰é¢æ­¥éª¤çš„è¯¦ç»†ç»“æœï¼š\n"
        for result in results:
            formatted += f"\næ­¥éª¤{result['step_number']}ï¼ˆ{result['title']}ï¼‰ï¼š\n{result['content']}\n"
        
        return formatted
    
    def _format_complete_thinking_results(self, results: list) -> str:
        """æ ¼å¼åŒ–æ‰€æœ‰æ€è€ƒç»“æœï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        formatted = ""
        for result in results:
            formatted += f"\næ­¥éª¤{result['step_number']}ï¼š{result['title']}\n{result['content']}\n"
        
        return formatted
    
    def _format_middleware_result(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸­é—´ä»¶ç»“æœ"""
        if not isinstance(result, dict):
            return str(result)
        
        formatted_response = "# ğŸ§  Sequential Thinking ä¸“ä¸šé€»è¾‘æ¨ç†åˆ†æ\n\n"
        
        # æ˜¾ç¤ºè§„åˆ’
        if "planning" in result:
            formatted_response += "### ğŸ“‹ æ€è€ƒè§„åˆ’\n"
            formatted_response += f"{result['planning']}\n\n"
        
        # æ˜¾ç¤ºæ­¥éª¤
        if "steps" in result and len(result["steps"]) > 0:
            formatted_response += "## ğŸ”¬ ä¸¥å¯†é€»è¾‘æ¨ç†è¿‡ç¨‹\n\n"
            formatted_response += "ä»¥ä¸‹æ˜¯å®Œæ•´çš„5æ­¥æ¨ç†åˆ†æï¼Œæ¯æ­¥ç®€æ´ä½†æ¸…æ™°åœ°å±•ç°æ¨ç†é€»è¾‘ï¼š\n\n"
            
            for step in result["steps"]:
                step_content = step.get('content', '').strip()
                if step_content:  # åªæ˜¾ç¤ºæœ‰å†…å®¹çš„æ­¥éª¤
                    formatted_response += f"### æ­¥éª¤ {step['step_number']}: {step['title']}\n\n"
                    formatted_response += f"{step_content}\n\n"
                    
                    # æ˜¾ç¤ºSTçŠ¶æ€
                    if step.get('st_state'):
                        st_state = step['st_state']
                        formatted_response += f"---\n*ğŸ”— æ¨ç†çŠ¶æ€ï¼šç¬¬{st_state.get('thoughtNumber')}/{st_state.get('totalThoughts')}æ­¥ | æ¨ç†å†å²é•¿åº¦ï¼š{st_state.get('thoughtHistoryLength')}*\n\n"
                else:
                    # å¦‚æœæ­¥éª¤å†…å®¹ä¸ºç©ºï¼Œæ˜¾ç¤ºè­¦å‘Š
                    formatted_response += f"### æ­¥éª¤ {step['step_number']}: {step['title']}\n\n"
                    formatted_response += f"âš ï¸ æ­¤æ­¥éª¤å†…å®¹ç”Ÿæˆå¼‚å¸¸ï¼Œè¯·é‡æ–°å°è¯•æ¨ç†ã€‚\n\n"
        else:
            # å¦‚æœæ²¡æœ‰æ­¥éª¤ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            formatted_response += "âš ï¸ **æ¨ç†æ­¥éª¤ç”Ÿæˆå¤±è´¥**\n\n"
            formatted_response += "Sequential Thinkingå·¥å…·æœªèƒ½ç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ­¥éª¤ã€‚è¿™å¯èƒ½æ˜¯ç”±äºï¼š\n"
            formatted_response += "1. LLMæœåŠ¡å¼‚å¸¸\n2. ç½‘ç»œè¿æ¥é—®é¢˜\n3. æ¨ç†å¤æ‚åº¦è¿‡é«˜\n\n"
        
        # æ˜¾ç¤ºæ€»ç»“
        if "summary" in result:
            formatted_response += "## ğŸ’¡ ç»¼åˆæ€»ç»“\n\n"
            formatted_response += f"{result['summary']}\n\n"
        
        # æ˜¾ç¤ºå…ƒä¿¡æ¯
        formatted_response += "---\n"
        formatted_response += f"**ğŸ¯ æ¨ç†æŠ€æœ¯è¯´æ˜**\n"
        formatted_response += f"- **æ–¹æ³•**ï¼šSequential Thinking ç»“æ„åŒ–æ¨ç†æ¡†æ¶\n"
        formatted_response += f"- **ç‰¹ç‚¹**ï¼šçŠ¶æ€ç®¡ç† + é€»è¾‘éªŒè¯ + å‡è®¾æ’é™¤\n"
        formatted_response += f"- **æ­¥æ•°**ï¼šå®Œæˆ {result.get('total_steps', 0)} æ­¥ä¸¥å¯†æ¨ç†\n"
        formatted_response += f"- **ä¼˜åŠ¿**ï¼šç›¸æ¯”ç›´è§‰åˆ¤æ–­ï¼Œæä¾›å¯éªŒè¯çš„æ¨ç†é“¾æ¡\n"
        
        return formatted_response 