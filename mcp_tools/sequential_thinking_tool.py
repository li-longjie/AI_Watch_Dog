import httpx
import requests
from typing import Dict, Any
from datetime import datetime
import traceback
from .base_tool import BaseMCPTool

class SequentialThinkingTool(BaseMCPTool):
    """Sequential Thinkingæ¨ç†å·¥å…·"""
    
    @property
    def tool_name(self) -> str:
        return "Sequential Thinkingæ¨ç†å·¥å…·"
    
    @property
    def description(self) -> str:
        return "ç”¨äºå¤æ‚é—®é¢˜çš„é€æ­¥æ€è€ƒå’Œæ¨ç†ï¼Œå¸®åŠ©å¤§æ¨¡å‹è¿›è¡Œç»“æ„åŒ–åˆ†æ"
    
    def get_available_functions(self) -> Dict[str, Dict[str, Any]]:
        return {
            "sequential_thinking": {
                "description": "å¯¹å¤æ‚é—®é¢˜è¿›è¡Œé€æ­¥æ€è€ƒå’Œæ¨ç†",
                "parameters": {
                    "prompt": {
                        "type": "string", 
                        "description": "éœ€è¦åˆ†æçš„é—®é¢˜æˆ–ä»»åŠ¡",
                        "required": True
                    },
                    "max_steps": {
                        "type": "integer",
                        "description": "æœ€å¤§æ€è€ƒæ­¥éª¤æ•°ï¼Œé»˜è®¤5æ­¥",
                        "required": False
                    }
                },
                "examples": [
                    "å¸®æˆ‘åˆ†æè¿™ä¸ªå¤æ‚çš„æ•°å­¦é—®é¢˜",
                    "é€æ­¥æ€è€ƒè¿™ä¸ªå•†ä¸šå†³ç­–",
                    "åˆ†æè¿™ä¸ªç¼–ç¨‹é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ",
                    "é€æ­¥æ¨ç†è¿™ä¸ªé€»è¾‘é—®é¢˜"
                ]
            }
        }
    
    async def execute_function(self, function_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒSequential ThinkingåŠŸèƒ½"""
        if function_name == "sequential_thinking":
            return await self._sequential_thinking(parameters)
        else:
            return {
                "status": "error",
                "message": f"æœªçŸ¥å‡½æ•°: {function_name}"
            }
    
    async def _sequential_thinking(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒSequential Thinkingæ¨ç†"""
        try:
            prompt = parameters.get("prompt")
            max_steps = parameters.get("max_steps", 5)
            
            if not prompt:
                return {
                    "status": "error",
                    "message": "ç¼ºå°‘å¿…éœ€å‚æ•°: prompt"
                }
            
            # è°ƒç”¨Sequential Thinkingä¸­é—´ä»¶
            result = await self._call_sequential_thinking_with_llm(prompt, max_steps)
            
            if "error" in result:
                return {
                    "status": "error",
                    "message": result["error"]
                }
            
            return {
                "status": "success",
                "thinking_result": result,
                "formatted_response": self._format_middleware_result(result)
            }
            
        except Exception as e:
            self.logger.error(f"Sequential Thinkingæ‰§è¡Œé”™è¯¯: {e}")
            return {
                "status": "error",
                "message": f"Sequential Thinkingæ‰§è¡Œå¤±è´¥: {str(e)}"
            }
    
    async def _call_sequential_thinking_with_llm(self, prompt: str, max_steps: int = 5) -> Dict[str, Any]:
        """ä½¿ç”¨Sequential Thinkingä½œä¸ºä¸­é—´ä»¶ååŠ©å¤§æ¨¡å‹æ¨ç†"""
        try:
            self.logger.info(f"å¯åŠ¨Sequential Thinkingè¾…åŠ©æ¨ç†: {prompt[:100]}...")
            
            st_endpoint = f"{self.base_url}/sequential-thinking/sequentialthinking"
            thinking_results = []
            
            # ä¿å­˜å®Œæ•´çš„åŸå§‹é—®é¢˜
            original_question = prompt
            
            # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨Sequential Thinkingè§„åˆ’æ€è€ƒæ­¥éª¤
            self.logger.info("=== é˜¶æ®µ1ï¼šSequential Thinkingè§„åˆ’æ€è€ƒæµç¨‹ ===")
            
            planning_prompt = f"""
åŸºäºä»¥ä¸‹å®Œæ•´é—®é¢˜ï¼Œè¯·è§„åˆ’{max_steps}ä¸ªé€æ­¥æ€è€ƒçš„æ­¥éª¤ï¼š

å®Œæ•´é—®é¢˜ï¼š{original_question}

è¯·ä¸ºæ¯ä¸ªæ­¥éª¤æä¾›ï¼š
1. æ­¥éª¤æ ‡é¢˜
2. è¯¥æ­¥éª¤éœ€è¦æ€è€ƒçš„å…·ä½“å†…å®¹
3. è¯¥æ­¥éª¤çš„é¢„æœŸè¾“å‡º

æ ¼å¼ï¼š
æ­¥éª¤1ï¼š[æ ‡é¢˜] - [æ€è€ƒå†…å®¹] - [é¢„æœŸè¾“å‡º]
æ­¥éª¤2ï¼š[æ ‡é¢˜] - [æ€è€ƒå†…å®¹] - [é¢„æœŸè¾“å‡º]
...
"""
            
            # è·å–æ€è€ƒè§„åˆ’ (è¿™é‡Œéœ€è¦è°ƒç”¨LLMæœåŠ¡)
            planning_result = await self._query_llm_model(planning_prompt)
            self.logger.info(f"æ€è€ƒè§„åˆ’å®Œæˆ: {planning_result[:200]}...")
            
            # è§£æè§„åˆ’ç»“æœ
            steps_plan = self._parse_thinking_plan(planning_result, max_steps)
            
            # ç¬¬äºŒé˜¶æ®µï¼šSequential ThinkingçŠ¶æ€ç®¡ç† + å¤§æ¨¡å‹æ‰§è¡Œ
            self.logger.info("=== é˜¶æ®µ2ï¼šæ‰§è¡Œç»“æ„åŒ–æ€è€ƒ ===")
            
            for step_num in range(1, max_steps + 1):
                # 2.1 å‘Sequential Thinkingæ³¨å†Œå½“å‰æ­¥éª¤
                st_request = {
                    "thought": f"æ‰§è¡Œæ­¥éª¤{step_num}ï¼š{steps_plan.get(step_num, {}).get('title', f'ç¬¬{step_num}æ­¥æ€è€ƒ')}",
                    "nextThoughtNeeded": step_num < max_steps,
                    "thoughtNumber": step_num,
                    "totalThoughts": max_steps,
                    "needsMoreThoughts": step_num < max_steps
                }
                
                try:
                    async with httpx.AsyncClient(timeout=60.0) as client:
                        st_response = await client.post(st_endpoint, json=st_request)
                        
                        if st_response.status_code == 200:
                            st_result = st_response.json()
                            self.logger.info(f"Sequential Thinkingæ­¥éª¤{step_num}æ³¨å†ŒæˆåŠŸï¼Œå†å²é•¿åº¦: {st_result.get('thoughtHistoryLength')}")
                            
                            # 2.2 ä½¿ç”¨å¤§æ¨¡å‹æ‰§è¡Œå…·ä½“æ€è€ƒ
                            step_info = steps_plan.get(step_num, {})
                            
                            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡
                            context = f"""
å®Œæ•´åŸå§‹é—®é¢˜ï¼š{original_question}

ä½ æ­£åœ¨è¿›è¡Œç¬¬{step_num}æ­¥æ€è€ƒï¼ˆå…±{max_steps}æ­¥ï¼‰ã€‚

å½“å‰æ­¥éª¤ï¼š{step_info.get('title', f'ç¬¬{step_num}æ­¥')}
æ€è€ƒé‡ç‚¹ï¼š{step_info.get('content', 'ç»§ç»­åˆ†æé—®é¢˜')}
é¢„æœŸè¾“å‡ºï¼š{step_info.get('expected', 'åˆ†æç»“æœ')}

{self._format_complete_previous_results(thinking_results) if thinking_results else ''}

è¯·åŸºäºå®Œæ•´çš„åŸå§‹é—®é¢˜å’Œå‰é¢æ­¥éª¤çš„ç»“æœï¼Œä¸“æ³¨äºå½“å‰æ­¥éª¤ï¼Œæä¾›è¯¦ç»†çš„åˆ†æå’Œå…·ä½“çš„è®¡ç®—ï¼š
"""
                            
                            step_result = await self._query_llm_model(context)
                            
                            # ä¿å­˜æ­¥éª¤ç»“æœ
                            thinking_results.append({
                                "step_number": step_num,
                                "title": step_info.get('title', f'ç¬¬{step_num}æ­¥'),
                                "content": step_result,
                                "st_state": st_result,
                                "timestamp": datetime.now().isoformat()
                            })
                            
                            self.logger.info(f"æ­¥éª¤{step_num}æ‰§è¡Œå®Œæˆ")
                            
                        else:
                            self.logger.warning(f"Sequential Thinkingæ­¥éª¤{step_num}æ³¨å†Œå¤±è´¥: {st_response.text}")
                            # å³ä½¿STå¤±è´¥ï¼Œä¹Ÿç»§ç»­ç”¨å¤§æ¨¡å‹æ‰§è¡Œ
                            step_info = steps_plan.get(step_num, {})
                            context = f"å®Œæ•´é—®é¢˜ï¼š{original_question}\nç¬¬{step_num}æ­¥ï¼š{step_info.get('content', 'ç»§ç»­åˆ†æ')}"
                            step_result = await self._query_llm_model(context)
                            
                            thinking_results.append({
                                "step_number": step_num,
                                "title": step_info.get('title', f'ç¬¬{step_num}æ­¥'),
                                "content": step_result,
                                "st_state": None,
                                "timestamp": datetime.now().isoformat()
                            })
                            
                except Exception as e:
                    self.logger.error(f"æ­¥éª¤{step_num}æ‰§è¡Œå¼‚å¸¸: {e}")
                    # å¤±è´¥æ—¶ä¹Ÿè¦ç»§ç»­
                    continue
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šç»¼åˆæ€»ç»“
            self.logger.info("=== é˜¶æ®µ3ï¼šç»¼åˆæ€»ç»“ ===")
            
            summary_prompt = f"""
åŸºäºä»¥ä¸‹é€æ­¥æ€è€ƒçš„ç»“æœï¼Œè¯·æä¾›ä¸€ä¸ªå®Œæ•´çš„æ€»ç»“ï¼š

åŸå§‹é—®é¢˜ï¼š{original_question}

æ€è€ƒè¿‡ç¨‹ï¼š
{self._format_complete_thinking_results(thinking_results)}

è¯·æä¾›ï¼š
1. å®Œæ•´çš„è§£ç­”ï¼ˆåŒ…å«å…·ä½“æ•°å€¼è®¡ç®—ï¼‰
2. å…³é”®çš„æ¨ç†æ­¥éª¤
3. æœ€ç»ˆç»“è®º
"""
            
            final_summary = await self._query_llm_model(summary_prompt)
            
            return {
                "original_prompt": original_question,
                "planning": planning_result,
                "steps": thinking_results,
                "summary": final_summary,
                "total_steps": len(thinking_results),
                "method": "sequential_thinking_middleware",
                "completed": True
            }
            
        except Exception as e:
            self.logger.error(f"Sequential Thinkingä¸­é—´ä»¶å¼‚å¸¸: {str(e)}")
            self.logger.error(traceback.format_exc())
            return {"error": f"Sequential Thinkingä¸­é—´ä»¶æ‰§è¡Œå¼‚å¸¸: {str(e)}"}
    
    async def _query_llm_model(self, prompt: str) -> str:
        """è°ƒç”¨LLMæ¨¡å‹ï¼ˆè¿™é‡Œéœ€è¦é›†æˆåˆ°ç°æœ‰çš„LLMæœåŠ¡ï¼‰"""
        try:
            # è¿™é‡Œéœ€è¦è°ƒç”¨ç°æœ‰çš„LLMæœåŠ¡
            # æš‚æ—¶è¿”å›ç®€åŒ–å“åº”ï¼Œç¨åä¼šé›†æˆåˆ°ä¸»ç³»ç»Ÿ
            from llm_service import chat_completion
            return await chat_completion(prompt)
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
    
    def _parse_thinking_plan(self, planning_text: str, max_steps: int) -> Dict[int, Dict[str, str]]:
        """è§£ææ€è€ƒè§„åˆ’"""
        steps = {}
        lines = planning_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('æ­¥éª¤') and 'ï¼š' in line:
                try:
                    # è§£æï¼šæ­¥éª¤1ï¼š[æ ‡é¢˜] - [å†…å®¹] - [é¢„æœŸ]
                    parts = line.split('ï¼š', 1)[1].split(' - ')
                    step_num = int(line.split('ï¼š')[0].replace('æ­¥éª¤', ''))
                    
                    if step_num <= max_steps:
                        steps[step_num] = {
                            'title': parts[0].strip() if len(parts) > 0 else f'ç¬¬{step_num}æ­¥',
                            'content': parts[1].strip() if len(parts) > 1 else 'ç»§ç»­åˆ†æ',
                            'expected': parts[2].strip() if len(parts) > 2 else 'åˆ†æç»“æœ'
                        }
                except:
                    continue
        
        # ç¡®ä¿æ‰€æœ‰æ­¥éª¤éƒ½æœ‰é»˜è®¤å€¼
        for i in range(1, max_steps + 1):
            if i not in steps:
                steps[i] = {
                    'title': f'ç¬¬{i}æ­¥æ€è€ƒ',
                    'content': 'ç»§ç»­åˆ†æé—®é¢˜',
                    'expected': 'åˆ†æç»“æœ'
                }
        
        return steps
    
    def _format_complete_previous_results(self, results: list) -> str:
        """æ ¼å¼åŒ–å‰é¢æ­¥éª¤çš„å®Œæ•´ç»“æœ"""
        if not results:
            return ""
        
        formatted = "å‰é¢æ­¥éª¤çš„è¯¦ç»†ç»“æœï¼š\n"
        for result in results:
            formatted += f"\næ­¥éª¤{result['step_number']}ï¼ˆ{result['title']}ï¼‰ï¼š\n{result['content']}\n"
        
        return formatted
    
    def _format_complete_thinking_results(self, results: list) -> str:
        """æ ¼å¼åŒ–æ‰€æœ‰æ€è€ƒç»“æœï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        formatted = ""
        for result in results:
            formatted += f"\næ­¥éª¤{result['step_number']}ï¼š{result['title']}\n{result['content']}\n"
        
        return formatted
    
    def _format_middleware_result(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸­é—´ä»¶ç»“æœ"""
        if not isinstance(result, dict):
            return str(result)
        
        formatted_response = "## ğŸ§  Sequential Thinking è¾…åŠ©æ¨ç†\n\n"
        
        # æ˜¾ç¤ºè§„åˆ’
        if "planning" in result:
            formatted_response += "### ğŸ“‹ æ€è€ƒè§„åˆ’\n"
            formatted_response += f"{result['planning']}\n\n"
        
        # æ˜¾ç¤ºæ­¥éª¤
        if "steps" in result:
            formatted_response += "### ğŸ”„ é€æ­¥æ‰§è¡Œ\n\n"
            for step in result["steps"]:
                formatted_response += f"**æ­¥éª¤{step['step_number']}ï¼š{step['title']}**\n\n"
                formatted_response += f"{step['content']}\n\n"
                
                # æ˜¾ç¤ºSTçŠ¶æ€ï¼ˆå¯é€‰ï¼‰
                if step.get('st_state'):
                    st_state = step['st_state']
                    formatted_response += f"*çŠ¶æ€ï¼šæ­¥éª¤{st_state.get('thoughtNumber')}/{st_state.get('totalThoughts')}ï¼Œå†å²é•¿åº¦{st_state.get('thoughtHistoryLength')}*\n\n"
        
        # æ˜¾ç¤ºæ€»ç»“
        if "summary" in result:
            formatted_response += "## ğŸ’¡ ç»¼åˆæ€»ç»“\n\n"
            formatted_response += f"{result['summary']}\n\n"
        
        # æ˜¾ç¤ºå…ƒä¿¡æ¯
        formatted_response += "---\n"
        formatted_response += f"*æ¨ç†æ–¹å¼ï¼šSequential Thinking ä¸­é—´ä»¶ + å¤§æ¨¡å‹API*\n"
        formatted_response += f"*å®Œæˆæ­¥æ•°ï¼š{result.get('total_steps', 0)}*\n"
        
        return formatted_response 