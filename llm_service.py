import httpx
from config import APIConfig
from typing import List
import json
import asyncio
import logging
from typing import Optional, Dict, Any, Union
import os

# ç¡…åŸºæµåŠ¨APIé…ç½®ï¼ˆç”¨äºŽRAGé—®ç­”ï¼‰- å·²åºŸå¼ƒï¼Œç»Ÿä¸€ä½¿ç”¨Chutes.ai
# SILICONFLOW_API_KEY = "sk-xugvbuiyayzzfeoelfytnfioimnwvzouawxlavixynzuloui"
# SILICONFLOW_API_URL = "https://api.siliconflow.cn/v1/chat/completions"
# SILICONFLOW_MODEL = "deepseek-ai/DeepSeek-V3"

class LLMService:
    @staticmethod
    async def get_response(prompt: str, use_chutes: bool = True) -> str:
        """è°ƒç”¨å¤§æ¨¡åž‹ç”Ÿæˆå›žç­”
        
        Args:
            prompt: æç¤ºè¯
            use_chutes: æ˜¯å¦ä½¿ç”¨Chutes.aiçš„æ¨¡åž‹ï¼ˆä¸ºäº†ä¿æŒæŽ¥å£å…¼å®¹æ€§ï¼Œé»˜è®¤ä¸ºTrueï¼‰
        """
        try:
            # ç»Ÿä¸€ä½¿ç”¨Chutes.aiçš„API
            return await chat_completion(prompt)
        except Exception as e:
            # ä½¿ç”¨ logging è®°å½•é”™è¯¯
            logging.error(f"è°ƒç”¨ LLMService.get_response æ—¶å‡ºé”™: {e}")
            # è¿”å›žå…·ä½“çš„é”™è¯¯ä¿¡æ¯
            return f"ç”Ÿæˆå›žç­”é”™è¯¯: {str(e)}"

    @staticmethod
    def format_response(response: str) -> str:
        """æ ¼å¼åŒ–æ¨¡åž‹å›žç­”"""
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ åŽå¤„ç†é€»è¾‘
        return response.strip()

# æ³¨é‡ŠæŽ‰ç¡…åŸºæµåŠ¨ç›¸å…³å‡½æ•°ï¼Œä¸å†ä½¿ç”¨
# async def query_siliconflow_model(prompt: str) -> str:
#     """è°ƒç”¨ç¡…åŸºæµåŠ¨APIèŽ·å–å›žç­”ï¼ˆç”¨äºŽæ™ºèƒ½é—®ç­”æ¨¡å—ï¼‰"""
#     try:
#         logging.info(f"æç¤ºè¯é•¿åº¦: {len(prompt)}")
#         
#         # æž„å»ºè¯·æ±‚æ•°æ®
#         request_data = {
#             "model": SILICONFLOW_MODEL,
#             "messages": [
#                 {"role": "user", "content": prompt}
#             ],
#             "stream": False,
#             "max_tokens": 512,
#             "temperature": 0.7,
#             "top_p": 0.7,
#             "top_k": 50,
#             "frequency_penalty": 0.5,
#             "n": 1,
#             "stop": []
#         }
#         
#         logging.info(f"æ­£åœ¨å‘é€è¯·æ±‚åˆ°ç¡…åŸºæµåŠ¨API")
#         
#         async with httpx.AsyncClient() as client:
#             response = await client.post(
#                 SILICONFLOW_API_URL,
#                 headers={
#                     "Authorization": f"Bearer {SILICONFLOW_API_KEY}",
#                     "Content-Type": "application/json"
#                 },
#                 json=request_data,
#                 timeout=60.0
#             )
#             
#             # è®°å½•å“åº”çŠ¶æ€ç 
#             logging.info(f"ç¡…åŸºæµåŠ¨APIå“åº”çŠ¶æ€ç : {response.status_code}")
#             
#             # æ£€æŸ¥HTTPé”™è¯¯
#             if response.status_code != 200:
#                 error_text = response.text
#                 logging.error(f"ç¡…åŸºæµåŠ¨APIè¿”å›žHTTPé”™è¯¯: {response.status_code} - {error_text}")
#                 
#                 # å¦‚æžœç¡…åŸºæµåŠ¨APIå¤±è´¥ï¼Œå°è¯•å›žé€€åˆ°Chutes.ai
#                 logging.info("ç¡…åŸºæµåŠ¨APIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Chutes.aiä½œä¸ºå›žé€€")
#                 return await chat_completion(prompt)
#             
#             # è§£æžå“åº”
#             result = response.json()
#             logging.info(f"ç¡…åŸºæµåŠ¨APIå“åº”: {result.keys()}")
#             
#             if "choices" in result and len(result["choices"]) > 0:
#                 message = result["choices"][0].get("message", {})
#                 content = message.get("content", "")
#                 return content
#             else:
#                 logging.error(f"ç¡…åŸºæµåŠ¨APIå“åº”ç¼ºå°‘choiceså­—æ®µ: {result}")
#                 
#                 # å¦‚æžœå“åº”æ ¼å¼å¼‚å¸¸ï¼Œå›žé€€åˆ°Chutes.ai
#                 logging.info("ç¡…åŸºæµåŠ¨APIå“åº”æ ¼å¼å¼‚å¸¸ï¼Œå°è¯•ä½¿ç”¨Chutes.aiä½œä¸ºå›žé€€")
#                 return await chat_completion(prompt)
#             
#     except Exception as e:
#         logging.error(f"è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡åž‹å¼‚å¸¸: {str(e)}")
#         
#         # å¦‚æžœå‘ç”Ÿå¼‚å¸¸ï¼Œå›žé€€åˆ°Chutes.ai
#         logging.info(f"ç¡…åŸºæµåŠ¨APIå¼‚å¸¸ï¼Œå°è¯•ä½¿ç”¨Chutes.aiä½œä¸ºå›žé€€: {e}")
#         return await chat_completion(prompt)

async def chat_completion(prompt: str, model: str = "deepseek", temperature: float = None, max_tokens: int = 1024) -> str:
    """ä½¿ç”¨Chutes.ai LLMæ¨¡åž‹ï¼ˆç”¨äºŽè§†é¢‘ç”»é¢æè¿°å’Œæ´»åŠ¨æå–ï¼‰
    
    Args:
        prompt: æç¤ºè¯
        model: æ¨¡åž‹åç§°ï¼Œå¯é€‰å€¼ä¸º 'deepseek' æˆ– 'qwen'
        temperature: æ¸©åº¦å‚æ•°ï¼Œå†³å®šè¾“å‡ºçš„éšæœºæ€§
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
    """
    # æ ¹æ®modelå‚æ•°é€‰æ‹©ä¸åŒçš„APIé…ç½®
    if model == "qwen":
        api_key = APIConfig.QWEN_API_KEY
        model_name = APIConfig.QWEN_MODEL
        api_url = APIConfig.QWEN_API_URL
    else:  # é»˜è®¤ä½¿ç”¨deepseek
        api_key = APIConfig.DEEPSEEK_API_KEY
        model_name = APIConfig.DEEPSEEK_MODEL
        api_url = APIConfig.DEEPSEEK_API_URL

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    data = {
        "model": model_name,
        "messages": [{
            "role": "user", 
            "content": prompt
        }],
        "max_tokens": max_tokens,
        "stream": False
        # å®Œå…¨ä¸å‘é€stopå‚æ•°ï¼Œé¿å…è§¦å‘é»˜è®¤åœæ­¢ä»¤ç‰Œ
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if temperature is not None:
        data["temperature"] = temperature

    logging.debug(f"ðŸš€ å‡†å¤‡è°ƒç”¨API: {api_url}")
    logging.debug(f"ðŸ“‹ ä½¿ç”¨æ¨¡åž‹: {model_name}")
    logging.debug(f"ðŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
    logging.debug(f"âš™ï¸ å‚æ•°: max_tokens={max_tokens}, temperature={temperature}")
    
    # æ£€æŸ¥æç¤ºè¯ä¸­æ˜¯å¦åŒ…å«å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—ç¬¦ï¼ˆæ³¨é‡ŠæŽ‰ä»¥å‡å°‘æ—¥å¿—å™ªéŸ³ï¼‰
    # if '#' in prompt:
    #     logging.warning(f"âš ï¸ è­¦å‘Šï¼šæç¤ºè¯åŒ…å«#å­—ç¬¦ï¼Œå¯èƒ½å¯¼è‡´APIåœæ­¢ä»¤ç‰Œé—®é¢˜")
    #     logging.debug(f"ðŸ“„ æç¤ºè¯é¢„è§ˆ: {prompt[:200]}...")

    try:
        async with httpx.AsyncClient(timeout=APIConfig.REQUEST_TIMEOUT) as client:
            response = await client.post(
                api_url,
                headers=headers,
                json=data
            )

        # æ£€æŸ¥HTTPçŠ¶æ€ç 
        if response.status_code != 200:
            error_message = f"Chutes.ai APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
            try:
                error_details = response.json()
                error_message += f" - {error_details}"
            except Exception:
                error_message += f" - å“åº”å†…å®¹: {response.text}"
            logging.error(error_message)
            return error_message

        # è§£æžæˆåŠŸå“åº”
        response_data = response.json()
        logging.debug(f"Chutes.ai API å®Œæ•´å“åº”: {response_data}")
        
        if "choices" in response_data and len(response_data["choices"]) > 0:
            message = response_data["choices"][0].get("message")
            if message:
                content = message.get("content")
                if isinstance(content, str):
                    logging.debug(f"æˆåŠŸèŽ·å–APIå“åº”å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                    return content.strip()
                else:
                    logging.error(f"APIå“åº”ä¸­contentéžå­—ç¬¦ä¸²æˆ–ä¸ºNone: {content}")
                    logging.error(f"å®Œæ•´messageå¯¹è±¡: {message}")
                    # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯æ˜¾ç¤ºAPIç¡®å®žè¢«è°ƒç”¨äº†
                    usage_info = response_data.get("usage", {})
                    logging.error(f"APIä½¿ç”¨æƒ…å†µ: {usage_info}")
                    return f"APIè¿”å›žçš„contentç±»åž‹é”™è¯¯: {type(content)}, å€¼: {content}"
            else:
                logging.error(f"APIå“åº”æ ¼å¼é”™è¯¯(ç¼ºå°‘message): {response_data}")
                return "é”™è¯¯ï¼šAPIå“åº”æ ¼å¼é”™è¯¯(message)"
        else:
            logging.error(f"Chutes.ai APIå“åº”æ ¼å¼é”™è¯¯(ç¼ºå°‘choices): {response_data}")
            return "APIå“åº”æ ¼å¼é”™è¯¯(choices)"

    except httpx.RequestError as e:
        logging.error(f"è¯·æ±‚Chutes.ai APIæ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
        return f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}"
    except Exception as e:
        logging.error(f"å¤„ç†Chutes.ai APIå“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return f"å¤„ç†å“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"

# Remove or comment out the specific deepseek_chat and qwen_chat functions
# if chat_completion now handles both.
# async def deepseek_chat(...): ...
# async def qwen_chat(...): ... 

async def get_llm_response(prompt: str, use_chutes: bool = True) -> str:
    """
    Wrapper function to call the LLM service.
    This function is provided for compatibility with modules expecting `get_llm_response`.
    It currently delegates to LLMService.get_response.
    """
    return await LLMService.get_response(prompt, use_chutes=use_chutes) 